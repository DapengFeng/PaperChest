@inproceedings{Liu2021TheMatterofTime,
  title        = {
    Brief Industry Paper: The Matter of Time — A General and Efficient System
    for Precise Sensor Synchronization in Robotic Computing
  },
  author       = {
    Liu, Shaoshan and Yu, Bo and Liu, Yahui and Zhang, Kunai and Qiao, Yisong
    and Li, Thomas Yuang and Tang, Jie and Zhu, Yuhao
  },
  year         = 2021,
  month        = {May},
  booktitle    = {
    2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium
    (RTAS)
  },
  pages        = {413--416},
  doi          = {10.1109/RTAS52030.2021.00040},
  issn         = {2642-7346},
  abstract     = {
    Time synchronization is a critical task in robotic computing such as
    autonomous driving. In the past few years, as we developed advanced robotic
    applications, our synchronization system has evolved as well. In this
    paper, we first introduce the time synchronization problem and explain the
    challenges of time synchronization, especially in robotic workloads.
    Summarizing these challenges, we then present a general hardware
    synchronization system for robotic computing, which delivers high
    synchronization accuracy while maintaining low energy and resource
    consumption. The proposed hardware synchronization system is a key building
    block in our future robotic products.
  }
}
@article{Yuan2021PixelLevelExtrinsicSelfCalibration,
  title        = {
    Pixel-Level Extrinsic Self Calibration of High Resolution LiDAR and Camera
    in Targetless Environments
  },
  author       = {Yuan, Chongjian and Liu, Xiyuan and Hong, Xiaoping and Zhang, Fu},
  year         = 2021,
  month        = {Oct},
  journal      = {IEEE Robotics and Automation Letters},
  volume       = 6,
  number       = 4,
  pages        = {7517--7524},
  doi          = {10.1109/LRA.2021.3098923},
  issn         = {2377-3766},
  abstract     = {
    In this letter, we present a novel method for automatic extrinsic
    calibration of high-resolution LiDARs and RGB cameras in targetless
    environments. Our approach does not require checkerboards but can achieve
    pixel-level accuracy by aligning natural edge features in the two sensors.
    On the theory level, we analyze the constraints imposed by edge features
    and the sensitivity of calibration accuracy with respect to edge
    distribution in the scene. On the implementation level, we carefully
    investigate the physical measuring principles of LiDARs and propose an
    efficient and accurate LiDAR edge extraction method based on point cloud
    voxel cutting and plane fitting. Due to the edges' richness in natural
    scenes, we have carried out experiments in many indoor and outdoor scenes.
    The results show that this method has high robustness, accuracy, and
    consistency. It can promote the research and application of the fusion
    between LiDAR and camera. We have open sourced our code on
    GitHub<sup>1</sup> to benefit the community.
  }
}
@article{Yang2017MonocularVisualIntertialStateEstimation,
  title        = {
    Monocular Visual–Inertial State Estimation With Online Initialization and
    Camera–IMU Extrinsic Calibration
  },
  author       = {Yang, Zhenfei and Shen, Shaojie},
  year         = 2017,
  month        = {Jan},
  journal      = {IEEE Transactions on Automation Science and Engineering},
  volume       = 14,
  number       = 1,
  pages        = {39--51},
  doi          = {10.1109/TASE.2016.2550621},
  issn         = {1558-3783},
  abstract     = {
    There have been increasing demands for developing microaerial vehicles with
    vision-based autonomy for search and rescue missions in complex
    environments. In particular, the monocular visual-inertial system (VINS),
    which consists of only an inertial measurement unit (IMU) and a camera,
    forms a great lightweight sensor suite due to its low weight and small
    footprint. In this paper, we address two challenges for rapid deployment of
    monocular VINS: 1) the initialization problem and 2) the calibration
    problem. We propose a methodology that is able to initialize velocity,
    gravity, visual scale, and camera-IMU extrinsic calibration on the fly. Our
    approach operates in natural environments and does not use any artificial
    markers. It also does not require any prior knowledge about the mechanical
    configuration of the system. It is a significant step toward plug-and-play
    and highly customizable visual navigation for mobile robots. We show
    through online experiments that our method leads to accurate calibration of
    camera-IMU transformation, with errors less than 0.02 m in translation and
    1° in rotation. We compare out method with a state-of-the-art marker-based
    offline calibration method and show superior results. We also demonstrate
    the performance of the proposed approach in large-scale indoor and outdoor
    experiments.
  }
}
