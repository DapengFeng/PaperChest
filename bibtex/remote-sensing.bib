@article{Aleissaee2022Transformers,
  title        = {Transformers in Remote Sensing: {A} Survey},
  author       = {
    Aleissaee, Abdulaziz Amer and Kumar, Amandeep and Anwer, Rao Muhammad and
    Khan, Salman and Cholakkal, Hisham  and Xia, Gui{-}Song and Khan, Fahad
    Shahbaz
  },
  year         = 2022,
  journal      = {CoRR},
  volume       = {abs/2209.01206},
  doi          = {10.48550/arXiv.2209.01206},
  url          = {https://doi.org/10.48550/arXiv.2209.01206},
  eprinttype   = {arXiv},
  eprint       = {2209.01206},
  timestamp    = {Mon, 26 Sep 2022 18:12:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2209-01206.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Lee2022C3Det,
  title        = {Interactive Multi-Class Tiny-Object Detection},
  author       = {
    Lee, Chunggi and Park, Seonwook and Song, Heon and Ryu, Jeongun and Kim,
    Sanghoon and Kim, Haejoon and Pereira, Sérgio and Yoo, Donggeun
  },
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {14116--14125},
  doi          = {10.1109/CVPR52688.2022.01374},
  issn         = {2575-7075},
  abstract     = {
    Annotating tens or hundreds of tiny objects in a given image is laborious
    yet crucial for a multitude of Computer Vision tasks. Such imagery
    typically contains objects from various categories, yet the multi-class
    interactive annotation setting for the detection task has thus far been
    unex-plored. To address these needs, we propose a novel interactive
    annotation method for multiple instances of tiny objects from multiple
    classes, based on a few point-based user in-puts. Our approach, C3Det,
    relates the full image context with annotator inputs in a local and global
    manner via late-fusion andfeature-correlation, respectively. We perform
    ex-periments on the Tiny-DOTA. and LCell datasets using both two-stage and
    one-stage object detection architectures to verify the efficacy of our
    approach. Our approach outper-forms existing approaches in interactive
    annotation, achieving higher mAP with fewer clicks. Furthermore, we
    validate the annotation efficiency of our approach in a user study where it
    is shown to be 2.85x faster and yield only 0.36x task load (NASA-TLX, lower
    is better) compared to manual annotation. The code is available at
    https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection.
  }
}
@article{Li2022DataFusion,
  title        = {
    Deep learning in multimodal remote sensing data fusion: A comprehensive
    review
  },
  author       = {
    Li, Jiaxin and Hong, Danfeng and Gao, Lianru and Yao, Jing and Zheng, Ke
    and Zhang, Bing and Chanussot, Jocelyn
  },
  year         = 2022,
  journal      = {International Journal of Applied Earth Observation and Geoinformation},
  volume       = 112,
  pages        = 102926,
  doi          = {https://doi.org/10.1016/j.jag.2022.102926},
  issn         = {1569-8432},
  url          = {https://www.sciencedirect.com/science/article/pii/S1569843222001248},
  keywords     = {
    Artificial intelligence, Data fusion, Deep learning, Multimodal, Remote
    sensing
  },
  abstract     = {
    With the extremely rapid advances in remote sensing (RS) technology, a
    great quantity of Earth observation (EO) data featuring considerable and
    complicated heterogeneity are readily available nowadays, which renders
    researchers an opportunity to tackle current geoscience applications in a
    fresh way. With the joint utilization of EO data, much research on
    multimodal RS data fusion has made tremendous progress in recent years, yet
    these developed traditional algorithms inevitably meet the performance
    bottleneck due to the lack of the ability to comprehensively analyze and
    interpret strongly heterogeneous data. Hence, this non-negligible
    limitation further arouses an intense demand for an alternative tool with
    powerful processing competence. Deep learning (DL), as a cutting-edge
    technology, has witnessed remarkable breakthroughs in numerous computer
    vision tasks owing to its impressive ability in data representation and
    reconstruction. Naturally, it has been successfully applied to the field of
    multimodal RS data fusion, yielding great improvement compared with
    traditional methods. This survey aims to present a systematic overview in
    DL-based multimodal RS data fusion. More specifically, some essential
    knowledge about this topic is first given. Subsequently, a literature
    survey is conducted to analyze the trends of this field. Some prevalent
    sub-fields in the multimodal RS data fusion are then reviewed in terms of
    the to-be-fused data modalities, i.e., spatiospectral, spatiotemporal,
    light detection and ranging-optical, synthetic aperture radar-optical, and
    RS-Geospatial Big Data fusion. Furthermore, We collect and summarize some
    valuable resources for the sake of the development in multimodal RS data
    fusion. Finally, the remaining challenges and potential future directions
    are highlighted.
  }
}
@inproceedings{Nguyen2022HDR-DSP,
  title        = {Self-Supervised Super-Resolution for Multi-Exposure Push-Frame Satellites},
  author       = {
    Nguyen, Ngoc Long and Anger, Jérémy and Davy, Axel and Arias, Pablo and
    Facciolo, Gabriele
  },
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {1848--1858},
  doi          = {10.1109/CVPR52688.2022.00190},
  issn         = {2575-7075},
  abstract     = {
    Modern Earth observation satellites capture multi-exposure bursts of
    push-frame images that can be super-resolved via computational means. In
    this work, we propose a super-resolution method for such multi-exposure
    sequences, a problem that has received very little attention in the
    literature. The proposed method can handle the signal-dependent noise in
    the inputs, process sequences of any length, and be robust to inaccuracies
    in the exposure times. Furthermore, it can be trained end-to-end with
    self-supervision, without requiring ground truth high resolution frames,
    which makes it especially suited to handle real data. Central to our method
    are three key contributions: i) a base-detail decomposition for handling
    errors in the exposure times, ii) a noise-level-aware feature encoding for
    improved fusion of frames with varying signal-to-noise ratio and iii) a
    permutation invariant fusion strategy by temporal pooling operators. We
    evaluate the proposed method on synthetic and real data and show that it
    outperforms by a significant margin existing single-exposure approaches
    that we adapted to the multi-exposure case.
  }
}
@inproceedings{Toker2022DynamicEarthNet,
  title        = {
    DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change
    Segmentation
  },
  author       = {
    Toker, Aysim and Kondmann, Lukas and Weber, Mark and Eisenberger, Marvin
    and Camero, Andrés and Hu, Jingliang and Hoderlein, Ariadna Pregel and
    Şenaras, Çağlar and Davis, Timothy and Cremers, Daniel and Marchisio,
    Giovanni and Zhu, Xiao Xiang and Leal-Taixé, Laura
  },
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {21126--21135},
  doi          = {10.1109/CVPR52688.2022.02048},
  issn         = {2575-7075},
  abstract     = {
    Earth observation is a fundamental tool for monitoring the evolution of
    land use in specific areas of interest. Observing and precisely defining
    change, in this context, requires both time-series data and pixel-wise
    segmentations. To that end, we propose the DynamicEarthNet dataset that
    consists of daily, multi-spectral satellite observations of 75 selected
    areas of interest distributed over the globe with imagery from Planet Labs.
    These observations are paired with pixel-wise monthly semantic segmentation
    labels of 7 land use and land cover (LULC) classes. DynamicEarthNet is the
    first dataset that provides this unique combination of daily measurements
    and high-quality labels. In our experiments, we compare several established
    baselines that either utilize the daily observations as additional training
    data (semi-supervised learning) or multiple observations at once
    (spatio-temporal learning) as a point of reference for future research.
    Finally, we propose a new evaluation metric SCS that addresses the specific
    challenges associated with time-series semantic change segmentation. The
    data is available at: https://mediatum.ub.tum.de/1650201.
  }
}
@inproceedings{Workman2022GeospatialAttention,
  title        = {Revisiting Near/Remote Sensing with Geospatial Attention},
  author       = {
    Workman, Scott and Rafique, M. Usman and Blanton, Hunter and Jacobs, Nathan
  },
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {1768--1777},
  doi          = {10.1109/CVPR52688.2022.00182},
  issn         = {2575-7075},
  abstract     = {
    This work addresses the task of overhead image segmentation when auxiliary
    ground-level images are available. Recent work has shown that performing
    joint inference over these two modalities, often called near/remote
    sensing, can yield significant accuracy improvements. Extending this line
    of work, we introduce the concept of geospatial attention, a geometry-aware
    attention mechanism that explicitly considers the geospatial relationship
    between the pixels in a ground-level image and a geographic location. We
    propose an approach for computing geospatial attention that incorporates
    geometric features and the appearance of the overhead and ground-level
    imagery. We introduce a novel architecture for near/remote sensing that is
    based on geospatial attention and demonstrate its use for five segmentation
    tasks. The results demonstrate that our method significantly outperforms
    the previous state-of-the-art methods.
  }
}
@inproceedings{Zorzi2022PolyWorld,
  title        = {
    PolyWorld: Polygonal Building Extraction with Graph Neural Networks in
    Satellite Images
  },
  author       = {
    Zorzi, Stefano and Bazrafkan, Shabab and Habenschuss, Stefan and
    Fraundorfer, Friedrich
  },
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {1938--1947},
  doi          = {10.1109/CVPR52688.2022.00189},
  issn         = {2575-7075},
  abstract     = {
    While most state-of-the-art instance segmentation methods produce binary
    segmentation masks, geographic and cartographic applications typically
    require precise vector polygons of extracted objects instead of rasterized
    output. This paper introduces PolyWorld, a neural network that directly
    extracts building vertices from an image and connects them correctly to
    create precise polygons. The model predicts the connection strength between
    each pair of vertices using a graph neural network and estimates the
    assignments by solving a differentiable optimal transport problem.
    Moreover, the vertex positions are optimized by minimizing a combined
    segmentation and polygonal angle difference loss. PolyWorld significantly
    outperforms the state of the art in building polygonization and achieves
    not only notable quantitative results, but also produces visually pleasing
    building polygons. Code and trained weights are publicly available at
    https://thub.com/zorzis/yWorl-PoldPretrainedNetwork.
  }
}
@inproceedings{Akiva2022MATTER,
  title        = {
    Self-Supervised Material and Texture Representation Learning for Remote
    Sensing Tasks
  },
  author       = {Akiva, Peri and Purri, Matthew and Leotta, Matthew},
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {8193--8205},
  doi          = {10.1109/CVPR52688.2022.00803},
  issn         = {2575-7075},
  abstract     = {
    Self-supervised learning aims to learn image feature representations
    without the usage of manually annotated lbabels. It is often used as a
    precursor step to obtain useful initial network weights which contribute to
    faster convergence and superior performance of downstream tasks. While
    self-supervision allows one to reduce the domain gap between supervised and
    unsupervised learning without the usage of labels, the self-supervised
    objective still requires a strong inductive bias to downstream tasks for
    effective transfer learning. In this work, we present our material and
    texture based self-supervision method named MATTER (MATerial and TExture
    Representation Learning), which is inspired by classical material and
    texture methods. Material and texture can effectively describe any surface,
    including its tactile properties, color, and specularity. By extension,
    effective representation of material and texture can describe other
    semantic classes strongly associated with said material and texture. MATTER
    leverages multitemporal, spatially aligned remote sensing imagery over
    unchanged regions to learn invariance to illumination and viewing angle as
    a mechanism to achieve consistency of material and texture representation.
    We show that our self-supervision pre-training method allows for up to
    24.22% and 6.33% performance increase in unsupervised and finetuned setups,
    and up to 76% faster convergence on change detection, land cover
    classification, and semantic segmentation tasks. Code and dataset:
    https://github.com/periakiva/MATTER.
  }
}
@inproceedings{Bandara2022HyperTransformer,
  title        = {
    HyperTransformer: A Textural and Spectral Feature Fusion Transformer for
    Pansharpening
  },
  author       = {Bandara, Wele Gedara Chaminda and Patel, Vishal M.},
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {1757--1767},
  doi          = {10.1109/CVPR52688.2022.00181},
  issn         = {2575-7075},
  abstract     = {
    Pansharpening aims to fuse a registered high-resolution panchromatic image
    (PAN) with a low-resolution hyper-spectral image (LR-HSI) to generate an
    enhanced HSI with high spectral and spatial resolution. Existing
    pansharpening approaches neglect using an attention mechanism to transfer
    HR texture features from PAN to LR-HSI features, resulting in spatial and
    spectral distortions. In this paper, we present a novel attention mechanism
    for pansharpening called HyperTransformer, in which features of LR-HSI and
    PAN are formulated as queries and keys in a transformer, respectively.
    HyperTransformer consists of three main modules, namely two separate
    feature extractors for PAN and HSI, a multi-head feature soft-attention
    module, and a spatial-spectral feature fusion module. Such a network
    improves both spatial and spectral quality measures of the pansharpened HSI
    by learning cross-feature space dependencies and long-range details of PAN
    and LR-HSI. Furthermore, HyperTransformer can be utilized across multiple
    spatial scales at the backbone for obtaining improved performance.
    Extensive experiments conducted on three widely used datasets demonstrate
    that HyperTransformer achieves significant improvement over the
    state-of-the-art methods on both spatial and spectral quality measures.
    Implementation code and pretrained weights can be accessed at
    https://github.com/wgcban/HyperTransformer.
  }
}
@inproceedings{Li2022OrientedPepPoints,
  title        = {Oriented RepPoints for Aerial Object Detection},
  author       = {Li, Wentong and Chen, Yijie and Hu, Kaixuan and Zhu, Jianke},
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {1819--1828},
  doi          = {10.1109/CVPR52688.2022.00187},
  issn         = {2575-7075},
  abstract     = {
    In contrast to the generic object, aerial targets are often non-axis
    aligned with arbitrary orientations having the cluttered surroundings.
    Unlike the mainstreamed approaches regressing the bounding box
    orientations, this paper proposes an effective adaptive points learning
    approach to aerial object detection by taking advantage of the adaptive
    points representation, which is able to capture the geometric information
    of the arbitrary-oriented instances. To this end, three oriented conversion
    functions are presented to facilitate the classification and localization
    with accurate orientation. Moreover, we propose an effective quality
    assessment and sample assignment scheme for adaptive points learning toward
    choosing the representative oriented reppoints samples during training,
    which is able to capture the non-axis aligned features from adjacent
    objects or background noises. A spatial constraint is introduced to
    penalize the outlier points for roust adaptive learning. Experimental
    results on four challenging aerial datasets including DOTA, HRSC2016,
    UCAS-AOD and DIOR-R, demonstrate the efficacy of our proposed approach. The
    source code is availabel at:
    https://github.com/LiWentomng/OrientedRepPoints.
  }
}
@inproceedings{Mari2022Sat-NeRF,
  title        = {
    Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient
    Objects and Shadow Modeling Using RPC Cameras
  },
  author       = {Marí, Roger and Facciolo, Gabriele and Ehret, Thibaud},
  year         = 2022,
  month        = {June},
  booktitle    = {
    2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops (CVPRW)
  },
  pages        = {1310--1320},
  doi          = {10.1109/CVPRW56347.2022.00137},
  issn         = {2160-7516},
  abstract     = {
    We introduce the Satellite Neural Radiance Field (SatNeRF), a new
    end-to-end model for learning multi-view satellite photogrammetry in the
    wild. Sat-NeRF combines some of the latest trends in neural rendering with
    native satellite camera models, represented by rational polynomial
    coefficient (RPC) functions. The proposed method renders new views and
    infers surface models of similar quality to those obtained with traditional
    state-of-the-art stereo pipelines. Multi-date images exhibit significant
    changes in appearance, mainly due to varying shadows and transient objects
    (cars, vegetation). Robustness to these challenges is achieved by a
    shadow-aware irradiance model and uncertainty weighting to deal with
    transient phenomena that cannot be explained by the position of the sun. We
    evaluate Sat-NeRF using WorldView-3 images from different locations and
    stress the advantages of applying a bundle adjustment to the satellite
    camera models prior to training. This boosts the network performance and
    can optionally be used to extract additional cues for depth supervision.
  }
}
@inproceedings{Shi2022Beyond,
  title        = {
    Beyond Cross-view Image Retrieval: Highly Accurate Vehicle Localization
    Using Satellite Image
  },
  author       = {Shi, Yujiao and Li, Hongdong},
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {16989--16999},
  doi          = {10.1109/CVPR52688.2022.01650},
  issn         = {2575-7075},
  abstract     = {
    This paper addresses the problem of vehicle-mounted camera localization by
    matching a ground-level image with an overhead-view satellite map. Existing
    methods often treat this problem as cross-view image retrieval, and use
    learned deep features to match the ground-level query im-age to a partition
    (e.g., a small patch) of the satellite map. By these methods, the
    localization accuracy is limited by the partitioning density of the
    satellite map (often in the order of tens meters). Departing from the
    conventional wisdom of image retrieval, this paper presents a novel
    solution that can achieve highly-accurate localization. The key idea is to
    formulate the task as pose estimation and solve it by neural-net based
    optimization. Specifically, we design a two-branch CNN to extract robust
    features from the ground and satellite images, respectively. To bridge the
    vast cross-view domain gap, we resort to a Geometry Projection module that
    projects features from the satellite map to the ground-view, based on a
    relative camera pose. Aiming to minimize the differences between the
    projected features and the observed features, we employ a differentiable
    Levenberg-Marquardt (LM) module to search for the optimal camera pose
    iteratively. The entire pipeline is differen-tiable and runs end-to-end.
    Extensive experiments on standard autonomous vehicle localization datasets
    have confirmed the superiority of the proposed method. Notably, e.g.,
    starting from a coarse estimate of camera location within a wide region of
    40m × 40m, with an 80% likelihood our method quickly reduces the lateral
    location error to be within 5m on a new KITTI cross-view dataset.
  }
}
@article{Tian2022PPTCGAN,
  title        = {UAV-Satellite View Synthesis for Cross-View Geo-Localization},
  author       = {Tian, Xiaoyang and Shao, Jie and Ouyang, Deqiang and Shen, Heng Tao},
  year         = 2022,
  month        = {July},
  journal      = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume       = 32,
  number       = 7,
  pages        = {4804--4815},
  doi          = {10.1109/TCSVT.2021.3121987},
  issn         = {1558-2205},
  abstract     = {
    The goal of cross-view image matching based on geo-localization is to
    determine the location of a given ground-view image (front view) by
    matching it with a group of satellite-view images (vertical view) with
    geographic tags. Due to the rapid development of unmanned aerial vehicle
    (UAV) technology in recent years, it has provided a real viewpoint close to
    45 degrees (oblique view) to bridge the visual gap between views. However,
    existing methods ignore the direct geometric space correspondence of
    UAV-satellite views, and only use brute force for feature matching, leading
    to inferior performance. In this context, we propose an end-to-end
    cross-view matching method that integrates cross-view synthesis module and
    geo-localization module, which fully considers the spatial correspondence
    of UAV-satellite views and the surrounding area information. To be
    specific, the cross-view synthesis module includes two parts: the oblique
    view of UAV is first converted to the vertical view by perspective
    projection transformation (PPT), which makes the UAV image closer to the
    satellite image; then we use conditional generative adversarial nets (CGAN)
    to synthesize the UAV image with vertical view style, which is close to the
    real satellite image by learning the converted UAV as the input image and
    the real satellite image as the label. Geo-localization module refers to
    existing local pattern network (LPN), which explicitly considers the
    surrounding environment of the target building. These modules are
    integrated in a single architecture called PCL, which mutually reinforce
    each other. Our method is superior to the existing UAV-satellite cross-view
    methods, which improves by about 5%.
  }
}
@inproceedings{Christie2021MGP,
  title        = {Single View Geocentric Pose in the Wild},
  author       = {
    Christie, Gordon and Foster, Kevin and Hagstrom, Shea and Hager, Gregory D.
    and Brown, Myron Z.
  },
  year         = 2021,
  month        = {June},
  booktitle    = {
    2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops (CVPRW)
  },
  pages        = {1162--1171},
  doi          = {10.1109/CVPRW53098.2021.00127},
  issn         = {2160-7516},
  abstract     = {
    Current methods for Earth observation tasks such as semantic mapping, map
    alignment, and change detection rely on near-nadir images; however, often
    the first available images in response to dynamic world events such as
    natural disasters are oblique. These tasks are much more difficult for
    oblique images due to observed object parallax. There has been recent
    success in learning to regress an object’s geocentric pose, defined as
    height above ground and orientation with respect to gravity, by training
    with airborne lidar registered to satellite images. We present a model for
    this novel task that exploits affine invariance properties to outperform
    state of the art performance by a wide margin. We also address practical
    issues required to deploy this method in the wild for real-world
    applications. Our data and code are publicly available 1.
  }
}
@inproceedings{Hagstrom2021CORE3D,
  title        = {Cumulative Assessment for Urban 3D Modeling},
  author       = {
    Hagstrom, Shea and Pak, Hee Won and Ku, Stephanie and Wang, Sean and Hager,
    Gregory and Brown, Myron
  },
  year         = 2021,
  month        = {July},
  booktitle    = {2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS},
  pages        = {3261--3264},
  doi          = {10.1109/IGARSS47720.2021.9554754},
  issn         = {2153-7003},
  abstract     = {
    Urban 3D modeling from satellite images requires accurate semantic
    segmentation to delineate urban features, multiple view stereo for 3D
    reconstruction of surface heights, and 3D model fitting to produce compact
    models with accurate surface slopes. In this work, we present a cumulative
    assessment metric that succinctly captures error contributions from each of
    these components. We demonstrate our approach by providing challenging
    public datasets and extending two open source projects to provide an
    end-to-end 3D modeling baseline solution to stimulate further research and
    evaluation with a public leaderboard.
  }
}
@inproceedings{He2021accidentRiskMap,
  title        = {
    Inferring high-resolution traffic accident risk maps based on satellite
    imagery and GPS trajectories
  },
  author       = {
    He, Songtao and Sadeghi, Mohammad Amin and Chawla, Sanjay and Alizadeh,
    Mohammad and Balakrishnan, Hari and Madden, Samuel
  },
  year         = 2021,
  month        = {Oct},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages        = {11957--11965},
  doi          = {10.1109/ICCV48922.2021.01176},
  issn         = {2380-7504},
  abstract     = {
    Traffic accidents cost about 3% of the world’s GDP and are the leading
    cause of death in children and young adults. Accident risk maps are useful
    tools to monitor and mitigate accident risk. We present a technique to
    generate high-resolution (5 meters) accident risk maps. At this high
    resolution, accidents are sparse and risk estimation is limited by
    bias-variance trade-off. Prior accident risk maps either estimate
    low-resolution maps that are of low utility (high bias), or they use
    frequency-based estimation techniques that inaccurately predict where
    accidents actually happen (high variance). To improve this trade-off, we
    use an end-to-end deep architecture that can input satellite imagery, GPS
    trajectories, road maps and the history of accidents. Our evaluation on
    four metropolitan areas in the US with a total area of 7,488 km2 shows that
    our technique outperform prior work in terms of resolution and accuracy.
  }
}
@inproceedings{Li2021Sat2Vid,
  title        = {
    Sat2Vid: Street-view Panoramic Video Synthesis from a Single Satellite
    Image
  },
  author       = {
    Li, Zuoyue and Li, Zhenqiang and Cui, Zhaopeng and Qin, Rongjun and
    Pollefeys, Marc and Oswald, Martin R.
  },
  year         = 2021,
  month        = {Oct},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages        = {12416--12425},
  doi          = {10.1109/ICCV48922.2021.01221},
  issn         = {2380-7504},
  abstract     = {
    We present a novel method for synthesizing both temporally and
    geometrically consistent street-view panoramic video from a single
    satellite image and camera trajectory. Existing cross-view synthesis
    approaches focus on images, while video synthesis in such a case has not
    yet received enough attention. For geometrical and temporal consistency,
    our approach explicitly creates a 3D point cloud representation of the
    scene and maintains dense 3D-2D correspondences across frames that reflect
    the geometric scene configuration inferred from the satellite view. As for
    synthesis in the 3D space, we implement a cascaded network architecture
    with two hourglass modules to generate point-wise coarse and fine features
    from semantics and per-class latent vectors, followed by projection to
    frames and an up-sampling module to obtain the final realistic video. By
    leveraging computed correspondences, the produced street-view video frames
    adhere to the 3D geometric scene structure and maintain temporal
    consistency. Qualitative and quantitative experiments demonstrate superior
    results compared to other state-of-the-art synthesis approaches that either
    lack temporal consistency or realistic appearance. To the best of our
    knowledge, our work is the first one to synthesize cross-view images to
    videos..
  }
}
@article{Ding2021LCM,
  title        = {
    A Practical Cross-View Image Matching Method between UAV and Satellite for
    UAV-Based Geo-Localization
  },
  author       = {Ding, Lirong and Zhou, Ji and Meng, Lingxuan and Long, Zhiyong},
  year         = 2021,
  journal      = {Remote Sensing},
  volume       = 13,
  number       = 1,
  doi          = {10.3390/rs13010047},
  issn         = {2072-4292},
  url          = {https://www.mdpi.com/2072-4292/13/1/47},
  article-number = 47,
  abstract     = {
    Cross-view image matching has attracted extensive attention due to its huge
    potential applications, such as localization and navigation. Unmanned
    aerial vehicle (UAV) technology has been developed rapidly in recent years,
    and people have more opportunities to obtain and use UAV-view images than
    ever before. However, the algorithms of cross-view image matching between
    the UAV view (oblique view) and the satellite view (vertical view) are
    still in their beginning stage, and the matching accuracy is expected to be
    further improved when applied in real situations. Within this context, in
    this study, we proposed a cross-view matching method based on location
    classification (hereinafter referred to LCM), in which the similarity
    between UAV and satellite views is considered, and we implemented the
    method with the newest UAV-based geo-localization dataset
    (University-1652). LCM is able to solve the imbalance of the input sample
    number between the satellite images and the UAV images. In the training
    stage, LCM can simplify the retrieval problem into a classification problem
    and consider the influence of the feature vector size on the matching
    accuracy. Compared with one study, LCM shows higher accuracies, and
    Recall@K (K &isin; {1, 5, 10}) and the average precision (AP) were improved
    by 5&ndash;10%. The expansion of satellite-view images and multiple queries
    proposed by the LCM are capable of improving the matching accuracy during
    the experiment. In addition, the influences of different feature sizes on
    the LCM&rsquo;s accuracy are determined, and we found that 512 is the
    optimal feature size. Finally, the LCM model trained based on synthetic
    UAV-view images was evaluated in real-world situations, and the evaluation
    result shows that it still has satisfactory matching accuracy. The LCM can
    realize the bidirectional matching between the UAV-view image and the
    satellite-view image and can contribute to two applications: (i) UAV-view
    image localization (i.e., predicting the geographic location of UAV-view
    images based on satellite-view images with geo-tags) and (ii) UAV
    navigation (i.e., driving the UAV to the region of interest in the
    satellite-view image based on the flight record).
  }
}
@inproceedings{Garnot2021PASTIS,
  title        = {
    Panoptic Segmentation of Satellite Image Time Series with Convolutional
    Temporal Attention Networks
  },
  author       = {Fare Garnot, Vivien Sainte and Landrieu, Loic},
  year         = 2021,
  month        = {Oct},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages        = {4852--4861},
  doi          = {10.1109/ICCV48922.2021.00483},
  issn         = {2380-7504},
  abstract     = {
    Unprecedented access to multi-temporal satellite imagery has opened new
    perspectives for a variety of Earth observation tasks. Among them,
    pixel-precise panoptic segmentation of agricultural parcels has major
    economic and environmental implications. While researchers have explored
    this problem for single images, we argue that the complex temporal patterns
    of crop phenology are better addressed with temporal sequences of images.
    In this paper, we present the first end-to-end, single-stage method for
    panoptic segmentation of Satellite Image Time Series (SITS). This module
    can be combined with our novel image sequence encoding network which relies
    on temporal self-attention to extract rich and adaptive multi-scale
    spatiotemporal features. We also introduce PASTIS, the first open-access
    SITS dataset with panoptic annotations. We demonstrate the superiority of
    our encoder for semantic segmentation against multiple competing
    architectures, and set up the first state-of-the-art of panoptic
    segmentation of SITS. Our implementation and PASTIS are publicly available.
  }
}
@inproceedings{Gao2021SatMVS,
  title        = {
    Rational Polynomial Camera Model Warping for Deep Learning Based Satellite
    Multi-View Stereo Matching
  },
  author       = {Gao, Jian and Liu, Jin and Ji, Shunping},
  year         = 2021,
  month        = {Oct},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages        = {6128--6137},
  doi          = {10.1109/ICCV48922.2021.00609},
  issn         = {2380-7504},
  abstract     = {
    Satellite multi-view stereo (MVS) imagery is particularly suited for
    large-scale Earth surface reconstruction. Differing from the perspective
    camera model (pin-hole model) that is commonly used for close-range and
    aerial cameras, the cubic rational polynomial camera (RPC) model is the
    mainstream model for push-broom linear-array satellite cameras. However,
    the homography warping used in the prevailing learning based MVS methods is
    only applicable to pin-hole cameras. In order to apply the SOTA learning
    based MVS technology to the satellite MVS task for large-scale Earth
    surface reconstruction, RPC warping should be considered. In this work, we
    propose, for the first time, a rigorous RPC warping module. The rational
    polynomial coefficients are recorded as a tensor, and the RPC warping is
    formulated as a series of tensor transformations. Based on the RPC warping,
    we propose the deep learning based satellite MVS (SatMVS) framework for
    large-scale and wide depth range Earth surface reconstruction. We also
    introduce a large-scale satellite image dataset consisting of 519 5120×5120
    images, which we call the TLC SatMVS dataset. The satellite images were
    acquired from a three-line camera (TLC) that catches triple-view images
    simultaneously, forming a valuable supplement to the existing open-source
    WorldView-3 datasets with single-scanline images. Experiments show that the
    proposed RPC warping module and the SatMVS framework can achieve a superior
    reconstruction accuracy compared to the pin-hole fitting method and
    conventional MVS methods. Code and data are available at
    https://github.com/WHU-GPCV/SatMVS.
  }
}
@inproceedings{Han2021ReDet,
  title        = {ReDet: A Rotation-equivariant Detector for Aerial Object Detection},
  author       = {Han, Jiaming and Ding, Jian and Xue, Nan and Xia, Gui-Song},
  year         = 2021,
  month        = {June},
  booktitle    = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {2785--2794},
  doi          = {10.1109/CVPR46437.2021.00281},
  issn         = {2575-7075},
  abstract     = {
    Recently, object detection in aerial images has gained much attention in
    computer vision. Different from objects in natural images, aerial objects
    are often distributed with arbitrary orientation. Therefore, the detector
    requires more parameters to encode the orientation information, which are
    often highly redundant and inefficient. Moreover, as ordinary CNNs do not
    explicitly model the orientation variation, large amounts of rotation
    augmented data is needed to train an accurate object detector. In this
    paper, we propose a Rotation-equivariant Detector (ReDet) to address these
    issues, which explicitly encodes rotation equivariance and rotation
    invariance. More precisely, we incorporate rotation-equivariant networks
    into the detector to extract rotation-equivariant features, which can
    accurately predict the orientation and lead to a huge reduction of model
    size. Based on the rotation-equivariant features, we also present
    Rotation-invariant RoI Align (RiRoI Align), which adaptively extracts
    rotation-invariant features from equivariant features according to the
    orientation of RoI. Extensive experiments on several challenging aerial
    image datasets DOTA-v1.0, DOTA-v1.5 and HRSC2016, show that our method can
    achieve state-of-the-art performance on the task of aerial object
    detection. Compared with previous best results, our ReDet gains 1.2, 3.5
    and 2.6 mAP on DOTA-v1.0, DOTA-v1.5 and HRSC2016 respectively while
    reducing the number of parameters by 60% (313 Mb vs. 121 Mb). The code is
    available at: https://github.com/csuhan/ReDet.
  }
}
@inproceedings{Lee2021SIPSA-Net,
  title        = {
    SIPSA-Net: Shift-Invariant Pan Sharpening with Moving Object Alignment for
    Satellite Imagery
  },
  author       = {Lee, Jaehyup and Seo, Soomin and Kim, Munchurl},
  year         = 2021,
  month        = {June},
  booktitle    = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {10161--10169},
  doi          = {10.1109/CVPR46437.2021.01003},
  issn         = {2575-7075},
  abstract     = {
    Pan-sharpening is a process of merging a high-resolution (HR) panchromatic
    (PAN) image and its corresponding low-resolution (LR) multi-spectral (MS)
    image to create an HR-MS and pan-sharpened image. However, due to the
    different sensors’ locations, characteristics and acquisition time, PAN and
    MS image pairs often tend to have various amounts of misalignment.
    Conventional deep-learning-based methods that were trained with such
    misaligned PAN-MS image pairs suffer from diverse artifacts such as
    double-edge and blur artifacts in the resultant PAN-sharpened images. In
    this paper, we propose a novel framework called shift-invariant
    pan-sharpening with moving object alignment (SIPSA-Net) which is the first
    method to take into account such large misalignment of moving object
    regions for PAN sharpening. The SISPA-Net has a feature alignment module
    (FAM) that can adjust one feature to be aligned to another feature, even
    between the two different PAN and MS domains. For better alignment in
    pan-sharpened images, a shift-invariant spectral loss is newly designed,
    which ignores the inherent misalignment in the original MS input, thereby
    having the same effect as optimizing the spectral loss with a well-aligned
    MS image. Extensive experimental results show that our SIPSA-Net can
    generate pan-sharpened images with remarkable improvements in terms of
    visual quality and alignment, compared to the state-of-the-art methods.
  }
}
@article{Minetto2021COVID-19,
  title        = {
    Measuring Human and Economic Activity From Satellite Imagery to Support
    City-Scale Decision-Making During COVID-19 Pandemic
  },
  author       = {Minetto, R. and Segundo, M. and Rotich, G. and Sarkar, S.},
  year         = 2021,
  month        = {jan},
  journal      = {IEEE Transactions on Big Data},
  publisher    = {IEEE Computer Society},
  address      = {Los Alamitos, CA, USA},
  volume       = 7,
  number       = {01},
  pages        = {56--68},
  doi          = {10.1109/TBDATA.2020.3032839},
  issn         = {2332-7790},
  abstract     = {
    The COVID-19 outbreak forced governments worldwide to impose lockdowns and
    quarantines to prevent virus transmission. As a consequence, there are
    disruptions in human and economic activities all over the globe. The
    recovery process is also expected to be rough. Economic activities impact
    social behaviors, which leave signatures in satellite images that can be
    automatically detected and classified. Satellite imagery can support the
    decision-making of analysts and policymakers by providing a different kind
    of visibility into the unfolding economic changes. In this article, we use
    a deep learning approach that combines strategic location sampling and an
    ensemble of lightweight convolutional neural networks (CNNs) to recognize
    specific elements in satellite images that could be used to compute
    economic indicators based on it, automatically. This CNN ensemble framework
    ranked third place in the US Department of Defense xView challenge, the
    most advanced benchmark for object detection in satellite images. We show
    the potential of our framework for temporal analysis using the US IARPA
    Function Map of the World (fMoW) dataset. We also show results on real
    examples of different sites before and after the COVID-19 outbreak to
    illustrate different measurable indicators. Our code and annotated
    high-resolution aerial scenes before and after the outbreak are available
    on GitHub.&lt;xref rid&#x3D;&quot;fn1&quot;
    ref-type&#x3D;&quot;fn&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/xref&gt;&lt;fn
    id&#x3D;&quot;fn1&quot;&gt;&lt;label&gt;1.&lt;/label&gt;&lt;p&gt;&lt;uri&gt;https://github.com/maups/covid19-satellite-analysis&lt;/uri&gt;.&lt;/p&gt;
    &lt;/fn&gt;
  },
  keywords     = {satellites;covid-19;economics;big data;buildings;pandemics}
}
@article{Qin20213DR,
  title        = {3D Reconstruction through Fusion of Cross-View Images},
  author       = {Qin, Rongjun and Song, Shuang and Ling, Xiao and Elhashash, Mostafa},
  year         = 2021,
  journal      = {CoRR},
  volume       = {abs/2106.14306},
  url          = {https://arxiv.org/abs/2106.14306},
  eprinttype   = {arXiv},
  eprint       = {2106.14306},
  timestamp    = {Wed, 30 Jun 2021 16:14:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-14306.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Toker2021CDE,
  title        = {
    Coming Down to Earth: Satellite-to-Street View Synthesis for
    Geo-Localization
  },
  author       = {Toker, Aysim and Zhou, Qunjie and Maximov, Maxim and Leal-Taixé, Laura},
  year         = 2021,
  month        = {June},
  booktitle    = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {6484--6493},
  doi          = {10.1109/CVPR46437.2021.00642},
  issn         = {2575-7075},
  abstract     = {
    The goal of cross-view image based geo-localization is to determine the
    location of a given street view image by matching it against a collection
    of geo-tagged satellite images. This task is notoriously challenging due to
    the drastic viewpoint and appearance differences between the two domains.
    We show that we can address this discrepancy explicitly by learning to
    synthesize realistic street views from satellite inputs. Following this
    observation, we propose a novel multi-task architecture in which image
    synthesis and retrieval are considered jointly. The rationale behind this
    is that we can bias our network to learn latent feature representations
    that are useful for retrieval if we utilize them to generate images across
    the two input domains. To the best of our knowledge, ours is the first
    approach that creates realistic street views from satellite images and
    localizes the corresponding query street-view simultaneously in an
    end-to-end manner. In our experiments, we obtain state-of-the-art
    performance on the CVUSA and CVACT benchmarks. Finally, we show compelling
    qualitative results for satellite-to-street view synthesis.
  }
}
@article{Yang2021R3Det,
  title        = {
    R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating
    Object
  },
  author       = {Yang, Xue and Yan, Junchi and Feng, Ziming and He, Tao},
  year         = 2021,
  month        = {May},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume       = 35,
  number       = 4,
  pages        = {3163--3171},
  doi          = {10.1609/aaai.v35i4.16426},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/16426},
  abstractnote = {
    Rotation detection is a challenging task due to the difficulties of
    locating the multi-angle objects and separating them effectively from the
    background. Though considerable progress has been made, for practical
    settings, there still exist challenges for rotating objects with large
    aspect ratio, dense distribution and category extremely imbalance. In this
    paper, we propose an end-to-end refined single-stage rotation detector for
    fast and accurate object detection by using a progressive regression
    approach from coarse to fine granularity. Considering the shortcoming of
    feature misalignment in existing refined single-stage detector, we design a
    feature refinement module to improve detection performance by getting more
    accurate features. The key idea of feature refinement module is to
    re-encode the position information of the current refined bounding box to
    the corresponding feature points through pixel-wise feature interpolation
    to realize feature reconstruction and alignment. For more accurate rotation
    estimation, an approximate SkewIoU loss is proposed to solve the problem
    that the calculation of SkewIoU is not derivable. Experiments on three
    popular remote sensing public datasets DOTA, HRSC2016, UCAS-AOD as well as
    one scene text dataset ICDAR2015 show the effectiveness of our approach.
    The source code is available at
    https://github.com/Thinklab-SJTU/R3Det_Tensorflow and is also integrated in
    our open source rotation detection benchmark:
    https://github.com/yangxue0827/RotationDetection.
  }
}
@article{Zhuang2021MSBA,
  title        = {
    A Faster and More Effective Cross-View Matching Method of UAV and Satellite
    Images for UAV Geolocalization
  },
  author       = {Zhuang, Jiedong and Dai, Ming and Chen, Xuruoyan and Zheng, Enhui},
  year         = 2021,
  journal      = {Remote Sensing},
  volume       = 13,
  number       = 19,
  doi          = {10.3390/rs13193979},
  issn         = {2072-4292},
  url          = {https://www.mdpi.com/2072-4292/13/19/3979},
  article-number = 3979,
  abstract     = {
    Cross-view geolocalization matches the same target in different images from
    various views, such as views of unmanned aerial vehicles (UAVs) and
    satellites, which is a key technology for UAVs to autonomously locate and
    navigate without a positioning system (e.g., GPS and GNSS). The most
    challenging aspect in this area is the shifting of targets and nonuniform
    scales among different views. Published methods focus on extracting coarse
    features from parts of images, but neglect the relationship between
    different views, and the influence of scale and shifting. To bridge this
    gap, an effective network is proposed with well-designed structures,
    referred to as multiscale block attention (MSBA), based on a local pattern
    network. MSBA cuts images into several parts with different scales, among
    which self-attention is applied to make feature extraction more efficient.
    The features of different views are extracted by a multibranch structure,
    which was designed to make different branches learn from each other,
    leading to a more subtle relationship between views. The method was
    implemented with the newest UAV-based geolocalization dataset. Compared
    with the existing state-of-the-art (SOTA) method, MSBA accuracy improved by
    almost 10% when the inference time was equal to that of the SOTA method;
    when the accuracy of MSBA was the same as that of the SOTA method,
    inference time was shortened by 30%.
  }
}
@inproceedings{Alajaji2020FSL,
  title        = {Few-Shot Learning For Remote Sensing Scene Classification},
  author       = {
    Alajaji, Dalal and Alhichri, Haikel S. and Ammour, Nassim and Alajlan, Naif
  },
  year         = 2020,
  month        = {March},
  booktitle    = {
    2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium
    (M2GARSS)
  },
  pages        = {81--84},
  doi          = {10.1109/M2GARSS47143.2020.9105154},
  abstract     = {
    Scene classification has become an important research topic in remote
    sensing (RS) field. Typical solution relies on labeling a large enough set
    of the RS scenes manually using expert opinion if needed, then training the
    algorithm on this set to learn how to correctly classify other new scenes.
    The best performance deep learning models required a large labeled dataset
    for training. Accordingly, there is great need to develop intelligent
    machine learning algorithm that can learn to classify RS datasets
    containing new unseen classes from few labeled samples only. This problem
    is known as few-shot machine learning. In this work we develop a deep
    few-shot learning method for the classification of RS scenes. The proposed
    method is based on prototypical deep neural networks combined with
    SqueezeNet pre-trained CNN for image embedding. In this paper, we report
    preliminary results using the two RS scene datasets UC Merced and
    optimal31.
  }
}
@inproceedings{Christie2020LGP,
  title        = {Learning Geocentric Object Pose in Oblique Monocular Images},
  author       = {
    Christie, Gordon and Munoz Abujder, Rodrigo Rene Rai and Foster, Kevin and
    Hagstrom, Shea and Hager, Gregory D. and Brown, Myron Z.
  },
  year         = 2020,
  month        = {June},
  booktitle    = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {14500--14508},
  doi          = {10.1109/CVPR42600.2020.01452},
  issn         = {2575-7075},
  abstract     = {
    An object's geocentric pose, defined as the height above ground and
    orientation with respect to gravity, is a powerful representation of
    real-world structure for object detection, segmentation, and localization
    tasks using RGBD images. For close-range vision tasks, height and
    orientation have been derived directly from stereo-computed depth and more
    recently from monocular depth predicted by deep networks. For long-range
    vision tasks such as Earth observation, depth cannot be reliably estimated
    with monocular images. Inspired by recent work in monocular height above
    ground prediction and optical flow prediction from static images, we
    develop an encoding of geocentric pose to address this challenge and train
    a deep network to compute the representation densely, supervised by
    publicly available airborne lidar. We exploit these attributes to rectify
    oblique images and remove observed object parallax to dramatically improve
    the accuracy of localization and to enable accurate alignment of multiple
    images taken from very different oblique viewpoints. We demonstrate the
    value of our approach by extending two large-scale public datasets for
    semantic segmentation in oblique satellite images. All of our data and code
    are publicly available.
  }
}
@inproceedings{Lu2020s2g,
  title        = {Geometry-Aware Satellite-to-Ground Image Synthesis for Urban Areas},
  author       = {
    Lu, Xiaohu and Li, Zuoyue and Cui, Zhaopeng and Oswald, Martin R. and
    Pollefeys, Marc and Qin, Rongjun
  },
  year         = 2020,
  month        = {June},
  booktitle    = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {856--864},
  doi          = {10.1109/CVPR42600.2020.00094},
  issn         = {2575-7075},
  abstract     = {
    We present a novel method for generating panoramic street-view images which
    are geometrically consistent with a given satellite image. Different from
    existing approaches that completely rely on a deep learning architecture to
    generalize cross-view image distributions, our approach explicitly loops in
    the geometric configuration of the ground objects based on the satellite
    views, such that the produced ground view synthesis preserves the geometric
    shape and the semantics of the scene. In particular, we propose a neural
    network with a geo-transformation layer that turns predicted ground-height
    values from the satellite view to a ground view while retaining the
    physical satellite-to-ground relation. Our results show that the
    synthesized image retains well-articulated and authentic geometric shapes,
    as well as texture richness of the street-view in various scenarios. Both
    qualitative and quantitative results demonstrate that our method compares
    favorably to other state-of-the-art approaches that lack geometric
    consistency.
  }
}
@inproceedings{Garnot2020psetae,
  title        = {
    Satellite Image Time Series Classification With Pixel-Set Encoders and
    Temporal Self-Attention
  },
  author       = {
    Sainte Fare Garnot, Vivien and Landrieu, Loic and Giordano, Sebastien and
    Chehata, Nesrine
  },
  year         = 2020,
  month        = {June},
  booktitle    = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {12322--12331},
  doi          = {10.1109/CVPR42600.2020.01234},
  issn         = {2575-7075},
  abstract     = {
    Satellite image time series, bolstered by their growing availability, are
    at the forefront of an extensive effort towards automated Earth monitoring
    by international institutions. In particular, large-scale control of
    agricultural parcels is an issue of major political and economic
    importance. In this regard, hybrid convolutional-recurrent neural
    architectures have shown promising results for the automated classification
    of satellite image time series. We propose an alternative approach in which
    the convolutional layers are advantageously replaced with encoders
    operating on unordered sets of pixels to exploit the typically coarse
    resolution of publicly available satellite images. We also propose to
    extract temporal features using a bespoke neural architecture based on
    self-attention instead of recurrent networks. We demonstrate experimentally
    that our method not only outperforms previous state-of-the-art approaches
    in terms of precision, but also significantly decreases processing time and
    memory requirements. Lastly, we release a large open-access annotated
    dataset as a benchmark for future work on satellite image time series.
  }
}
@article{Antoniou2020DisasterResponse,
  title        = {A Deep Learning Method to Accelerate the Disaster Response Process},
  author       = {Antoniou, Vyron and Potsiou, Chryssy},
  year         = 2020,
  journal      = {Remote Sensing},
  volume       = 12,
  number       = 3,
  doi          = {10.3390/rs12030544},
  issn         = {2072-4292},
  url          = {https://www.mdpi.com/2072-4292/12/3/544},
  article-number = 544,
  abstract     = {
    This paper presents an end-to-end methodology that can be used in the
    disaster response process. The core element of the proposed method is a
    deep learning process which enables a helicopter landing site analysis
    through the identification of soccer fields. The method trains a deep
    learning autoencoder with the help of volunteered geographic information
    and satellite images. The process is mostly automated, it was developed to
    be applied in a time- and resource-constrained environment and keeps the
    human factor in the loop in order to control the final decisions. We show
    that through this process the cognitive load (CL) for an expert image
    analyst will be reduced by 70%, while the process will successfully
    identify 85.6% of the potential landing sites. We conclude that the
    suggested methodology can be used as part of a disaster response process.
  }
}
@inproceedings{Bosch2019US3D,
  title        = {Semantic Stereo for Incidental Satellite Images},
  author       = {
    Bosch, Marc and Foster, Kevin and Christie, Gordon and Wang, Sean and
    Hager, Gregory D. and Brown, Myron
  },
  year         = 2019,
  month        = {Jan},
  booktitle    = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages        = {1524--1532},
  doi          = {10.1109/WACV.2019.00167},
  issn         = {1550-5790},
  abstract     = {
    The increasingly common use of incidental satellite images for stereo
    reconstruction versus rigidly tasked binocular or trinocular coincident
    collection is helping to enable timely global-scale 3D mapping; however,
    reliable stereo correspondence from multi-date image pairs remains very
    challenging due to seasonal appearance differences and scene change.
    Promising recent work suggests that semantic scene segmentation can provide
    a robust regularizing prior for resolving ambiguities in stereo
    correspondence and reconstruction problems. To enable research for pairwise
    semantic stereo and multi-view semantic 3D reconstruction with incidental
    satellite images, we have established a large-scale public dataset
    including multi-view, multi-band satellite images and ground truth
    geometric and semantic labels for two large cities. To demonstrate the
    complementary nature of the stereo and segmentation tasks, we present
    lightweight public baselines adapted from recent state of the art
    convolutional neural network models and assess their performance.
  }
}
@inproceedings{Wier2019SpaceNetMVOI,
  title        = {SpaceNet MVOI: A Multi-View Overhead Imagery Dataset},
  author       = {
    Weir, Nicholas and Lindenbaum, David and Bastidas, Alexei and Etten, Adam
    and Kumar, Varun and Mcpherson, Sean and Shermeyer, Jacob and Tang, Hanlin
  },
  year         = 2019,
  month        = {Oct},
  booktitle    = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages        = {992--1001},
  doi          = {10.1109/ICCV.2019.00108},
  issn         = {2380-7504},
  abstract     = {
    Detection and segmentation of objects in overheard imagery is a challenging
    task. The variable density, random orientation, small size, and
    instance-to-instance heterogeneity of objects in overhead imagery calls for
    approaches distinct from existing models designed for natural scene
    datasets. Though new overhead imagery datasets are being developed, they
    almost universally comprise a single view taken from directly overhead (“at
    nadir”), failing to address a critical variable: look angle. By contrast,
    views vary in real-world overhead imagery, particularly in dynamic
    scenarios such as natural disasters where first looks are often over 40°
    off-nadir. This represents an important challenge to computer vision
    methods, as changing view angle adds distortions, alters resolution, and
    changes lighting. At present, the impact of these perturbations for
    algorithmic detection and segmentation of objects is untested. To address
    this problem, we present an open source Multi-View Overhead Imagery
    dataset, termed SpaceNet MVOI, with 27 unique looks from a broad range of
    viewing angles (-32.5° to 54.0°). Each of these images cover the same 665
    km2 geographic extent and are annotated with 126,747 building footprint
    labels, enabling direct assessment of the impact of viewpoint perturbation
    on model performance. We benchmark multiple leading segmentation and object
    detection models on: (1) building detection, (2) generalization to unseen
    viewing angles and resolutions, and (3) sensitivity of building footprint
    extraction to changes in resolution. We find that state of the art
    segmentation and object detection models struggle to identify buildings in
    off-nadir imagery and generalize poorly to unseen views, presenting an
    important benchmark to explore the broadly relevant challenge of detecting
    small, heterogeneous target objects in visually dynamic contexts.
  }
}
@inproceedings{Ding2019RoITrans,
  title        = {Learning RoI Transformer for Oriented Object Detection in Aerial Images},
  author       = {Ding, Jian and Xue, Nan and Long, Yang and Xia, Gui-Song and Lu, Qikai},
  year         = 2019,
  month        = {June},
  booktitle    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {2844--2853},
  doi          = {10.1109/CVPR.2019.00296},
  issn         = {2575-7075},
  abstract     = {
    Object detection in aerial images is an active yet challenging task in
    computer vision because of the bird’s-eye view perspective, the highly
    complex backgrounds, and the variant appearances of objects. Especially
    when detecting densely packed objects in aerial images, methods relying on
    horizontal proposals for common object detection often introduce mismatches
    between the Region of Interests (RoIs) and objects. This leads to the
    common misalignment between the final object classification confidence and
    localization accuracy. In this paper, we propose a RoI Transformer to
    address these problems. The core idea of RoI Transformer is to apply
    spatial transformations on RoIs and learn the transformation parameters
    under the supervision of oriented bounding box (OBB) annotations. RoI
    Transformer is with lightweight and can be easily embedded into detectors
    for oriented object detection. Simply apply the RoI Transformer to light
    head RCNN has achieved state-of-the-art performances on two common and
    challenging aerial datasets, i.e., DOTA and HRSC2016, with a neglectable
    reduction to detection speed. Our RoI Transformer exceeds the deformable
    Position Sensitive RoI pooling when oriented bounding-box annotations are
    available. Extensive experiments have also validated the flexibility and
    effectiveness of our RoI Transformer.
  }
}
@article{Gu2019RSISU,
  title        = {
    A Survey on Deep Learning-Driven Remote Sensing Image Scene Understanding:
    Scene Classification, Scene Retrieval and Scene-Guided Object Detection
  },
  author       = {Gu, Yating and Wang, Yantian and Li, Yansheng},
  year         = 2019,
  journal      = {Applied Sciences},
  volume       = 9,
  number       = 10,
  doi          = {10.3390/app9102110},
  issn         = {2076-3417},
  url          = {https://www.mdpi.com/2076-3417/9/10/2110},
  article-number = 2110,
  abstract     = {
    As a fundamental and important task in remote sensing, remote sensing image
    scene understanding (RSISU) has attracted tremendous research interest in
    recent years. RSISU includes the following sub-tasks: remote sensing image
    scene classification, remote sensing image scene retrieval, and
    scene-driven remote sensing image object detection. Although these
    sub-tasks have different goals, they share some communal hints. Hence, this
    paper tries to discuss them as a whole. Similar to other domains (e.g.,
    speech recognition and natural image recognition), deep learning has also
    become the state-of-the-art technique in RSISU. To facilitate the
    sustainable progress of RSISU, this paper presents a comprehensive review
    of deep-learning-based RSISU methods, and points out some future research
    directions and potential applications of RSISU.
  }
}
@inproceedings{Shetty2019UAVPoseEstimation,
  title        = {
    UAV Pose Estimation using Cross-view Geolocalization with Satellite Imagery
  },
  author       = {Shetty, Akshay and Gao, Grace Xingxin},
  year         = 2019,
  month        = {May},
  booktitle    = {2019 International Conference on Robotics and Automation (ICRA)},
  pages        = {1827--1833},
  doi          = {10.1109/ICRA.2019.8794228},
  issn         = {2577-087X},
  abstract     = {
    We propose an image-based cross-view geolocalization method that estimates
    the global pose of a UAV with the aid of georeferenced satellite imagery.
    Our method consists of two Siamese neural networks that extract relevant
    features despite large differences in viewpoints. The input to our method
    is an aerial UAV image and nearby satellite images, and the output is the
    weighted global pose estimate of the UAV camera. We also present a
    framework to integrate our crossview geolocalization output with visual
    odometry through a Kalman filter. We build a dataset of simulated UAV
    images and satellite imagery to train and test our networks. We show that
    our method performs better than previous camera pose estimation methods,
    and we demonstrate our networks ability to generalize well to test datasets
    with unseen images. Finally, we show that integrating our method with
    visual odometry significantly reduces trajectory estimation errors.
  }
}
@inproceedings{Christie2018fMoW,
  title        = {Functional Map of the World},
  author       = {Christie, Gordon and Fendley, Neil and Wilson, James and Mukherjee, Ryan},
  year         = 2018,
  month        = {June},
  booktitle    = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages        = {6172--6180},
  doi          = {10.1109/CVPR.2018.00646},
  issn         = {2575-7075},
  abstract     = {
    We present a new dataset, Functional Map of the World (fMoW), which aims to
    inspire the development of machine learning models capable of predicting
    the functional purpose of buildings and land use from temporal sequences of
    satellite images and a rich set of metadata features. The metadata provided
    with each image enables reasoning about location, time, sun angles,
    physical sizes, and other features when making predictions about objects in
    the image. Our dataset consists of over 1 million images from over 200
    countries. For each image, we provide at least one bounding box annotation
    containing one of 63 categories, including a "false detection" category. We
    present an analysis of the dataset along with baseline approaches that
    reason about metadata and temporal views. Our data, code, and pretrained
    models have been made publicly available.
  }
}
@article{Bosch2017MVS3DM,
  title        = {{Metric Evaluation Pipeline for 3d Modeling of Urban Scenes}},
  author       = {
    {Bosch}, M. and {Leichtman}, A. and {Chilcott}, D. and {Goldberg}, H. and
    {Brown}, M.
  },
  year         = 2017,
  month        = may,
  journal      = {
    ISPRS - International Archives of the Photogrammetry, Remote Sensing and
    Spatial Information Sciences
  },
  volume       = {42W1},
  pages        = {239--246},
  doi          = {10.5194/isprs-archives-XLII-1-W1-239-2017},
  adsurl       = {https://ui.adsabs.harvard.edu/abs/2017ISPAr42W1..239B},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Khanal2017PrecisionAgriculture,
  title        = {
    An overview of current and potential applications of thermal remote sensing
    in precision agriculture
  },
  author       = {Khanal, Sami and Fulton, John and Shearer, Scott},
  year         = 2017,
  journal      = {Computers and Electronics in Agriculture},
  volume       = 139,
  pages        = {22--32},
  doi          = {https://doi.org/10.1016/j.compag.2017.05.001},
  issn         = {0168-1699},
  url          = {https://www.sciencedirect.com/science/article/pii/S0168169916310225},
  keywords     = {Precision agriculture, Monitoring, Remote sensing, Thermal sensing},
  abstract     = {
    Precision agriculture (PA) utilizes tools and technologies to identify
    in-field soil and crop variability for improving farming practices and
    optimizing agronomic inputs. Traditionally, optical remote sensing (RS)
    that utilizes visible light and infrared regions of the electromagnetic
    spectrum has been used as an integral part of PA for crop and soil
    monitoring. Optical RS, however, is slow in differentiating stress levels
    in crops until visual symptoms become noticeable. Surface temperature is
    considered to be a rapid response variable that can indicate crop stresses
    prior to their visual symptoms. By measuring estimates of surface
    temperature, thermal RS has been found to be a promising tool for PA.
    Compared to optical RS, applications of thermal RS for PA have been
    limited. Until recently (i.e., before the advancement of low cost RS
    platforms such as unmanned aerial systems (UAVs)), the availability of high
    resolution thermal images was limited due to high acquisition costs. Given
    recent developments in UAVs, thermal images with high spatial and temporal
    resolutions have become available at a low cost, which has increased
    opportunities to understand in-field variability of crop and soil
    conditions useful for various agronomic decision-making. Before thermal RS
    is adopted as a routine tool for crop and environmental monitoring, there
    is a need to understand its current and potential applications as well as
    issues and concerns. This review focuses on current and potential
    applications of thermal RS in PA as well as some concerns relating to its
    application. The application areas of thermal RS in agriculture discussed
    here include irrigation scheduling, drought monitoring, crop disease
    detection, and mapping of soil properties, residues and tillage, field
    tiles, and crop maturity and yield. Some of the issues related to its
    application include spatial and temporal resolution, atmospheric
    conditions, and crop growth stages.
  }
}
