@inproceedings{Ding2022LargeKernelDesign,
  title        = {Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs},
  author       = {Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
  year         = 2022,
  month        = {June},
  booktitle    = {
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)
  },
  pages        = {11963--11975}
}
@inproceedings{Wang2021PE-BERT,
  title        = {On Position Embeddings in BERT},
  author       = {
    Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang
    and Qun Liu and Jakob Grue Simonsen
  },
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=onxoVA9FxMw}
}
@inproceedings{Alsallakh2021MindThePad,
  title        = {Mind the Pad -- CNNs Can Develop Blind Spots},
  author       = {
    Bilal Alsallakh and Narine Kokhlikyan and Vivek Miglani and Jun Yuan and
    Orion Reblitz-Richardson
  },
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=m1CD7tPubNy}
}
@inproceedings{Dosovitskiy2021ViT,
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author       = {
    Dosovitskiy, lexey and Beyer, Lucas and Kolesnikov, Alexander and
    Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani,
    Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and
    Uszkoreit, Jakob and Houlsby, Neil
  },
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=YicbFdNTTy}
}
@article{Islam2021Position,
  title        = {
    Position, padding and predictions: A deeper look at position information in
    cnns
  },
  author       = {
    Islam, Md Amirul and Kowal, Matthew and Jia, Sen and Derpanis, Konstantinos
    G and Bruce, Neil DB
  },
  year         = 2021,
  journal      = {arXiv preprint arXiv:2101.12322}
}
@inproceedings{Narang2021Transformer,
  title        = {
    Do Transformer Modifications Transfer Across Implementations and
    Applications?
  },
  author       = {
    Narang, Sharan  and Chung, Hyung Won  and Tay, Yi  and Fedus, Liam  and
    Fevry, Thibault  and Matena, Michael  and Malkan, Karishma  and Fiedel,
    Noah  and Shazeer, Noam  and Lan, Zhenzhong  and Zhou, Yanqi  and Li, Wei
    and Ding, Nan  and Marcus, Jake  and Roberts, Adam  and Raffel, Colin
  },
  year         = 2021,
  month        = nov,
  booktitle    = {
    Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing
  },
  publisher    = {Association for Computational Linguistics},
  address      = {Online and Punta Cana, Dominican Republic},
  pages        = {5758--5773},
  doi          = {10.18653/v1/2021.emnlp-main.465},
  url          = {https://aclanthology.org/2021.emnlp-main.465},
  abstract     = {
    The research community has proposed copious modifications to the
    Transformer architecture since it was introduced over three years ago,
    relatively few of which have seen widespread adoption. In this paper, we
    comprehensively evaluate many of these modifications in a shared
    experimental setting that covers most of the common uses of the Transformer
    in natural language processing. Surprisingly, we find that most
    modifications do not meaningfully improve performance. Furthermore, most of
    the Transformer variants we found beneficial were either developed in the
    same codebase that we used or are relatively minor changes. We conjecture
    that performance improvements may strongly depend on implementation details
    and correspondingly make some recommendations for improving the generality
    of experimental results.
  }
}
@inproceedings{Wu2021RPE,
  title        = {Rethinking and Improving Relative Position Encoding for Vision Transformer},
  author       = {
    Wu, Kan and Peng, Houwen and Chen, Minghao and Fu, Jianlong and Chao,
    Hongyang
  },
  year         = 2021,
  month        = {Oct},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages        = {10013--10021},
  doi          = {10.1109/ICCV48922.2021.00988},
  issn         = {2380-7504},
  abstract     = {
    Relative position encoding (RPE) is important for transformer to capture
    sequence ordering of input tokens. General efficacy has been proven in
    natural language processing. However, in computer vision, its efficacy is
    not well studied and even remains controversial, e.g., whether relative
    position encoding can work equally well as absolute position? In order to
    clarify this, we first review existing relative position encoding methods
    and analyze their pros and cons when applied in vision transformers. We
    then propose new relative position encoding methods dedicated to 2D images,
    called image RPE (iRPE). Our methods consider directional relative distance
    modeling as well as the interactions between queries and relative position
    embeddings in self-attention mechanism. The proposed iRPE methods are
    simple and lightweight. They can be easily plugged into transformer blocks.
    Experiments demonstrate that solely due to the proposed encoding methods,
    DeiT [21] and DETR [1] obtain up to 1.5% (top-1 Acc) and 1.3% (mAP) stable
    improvements over their original versions on ImageNet and COCO
    respectively, without tuning any extra hyperparameters such as learning
    rate and weight decay. Our ablation and analysis also yield interesting
    findings, some of which run counter to previous understanding. Code and
    models are open-sourced at
    https://github.com/microsoft/Cream/tree/main/iRPE.
  }
}
@inproceedings{Xu2021PEinGANs,
  title        = {Positional Encoding as Spatial Inductive Bias in GANs},
  author       = {
    Xu, Rui and Wang, Xintao and Chen, Kai and Zhou, Bolei and Loy, Chen Change
  },
  year         = 2021,
  month        = {June},
  booktitle    = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {13564--13573},
  doi          = {10.1109/CVPR46437.2021.01336},
  issn         = {2575-7075},
  abstract     = {
    SinGAN shows impressive capability in learning internal patch distribution
    despite its limited effective receptive field. We are interested in knowing
    how such a translationinvariant convolutional generator could capture the
    global structure with just a spatially i.i.d. input. In this work, taking
    SinGAN and StyleGAN2 as examples, we show that such capability, to a large
    extent, is brought by the implicit positional encoding when using zero
    padding in the generators. Such positional encoding is indispensable for
    generating images with high fidelity. The same phenomenon is observed in
    other generative architectures such as DCGAN and PGGAN. We further show
    that zero padding leads to an unbalanced spatial bias with a vague relation
    between locations. To offer a better spatial inductive bias, we investigate
    alternative positional encodings and analyze their effects. Based on a more
    flexible positional encoding explicitly, we propose a new multi-scale
    training strategy and demonstrate its effectiveness in the state-of-the-art
    unconditional generator StyleGAN2. Besides, the explicit spatial inductive
    bias substantially improves SinGAN for more versatile image manipulation. 1
  }
}
@inproceedings{He2021DEBERTA,
  title        = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author       = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=XPZIaotutsD}
}
@article{Raffel2020T5,
  title        = {
    Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer
  },
  author       = {
    Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and
    Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu,
    Peter J.
  },
  year         = 2020,
  journal      = {Journal of Machine Learning Research},
  volume       = 21,
  number       = 140,
  pages        = {1--67},
  url          = {http://jmlr.org/papers/v21/20-074.html}
}
@article{Scardapane2020CVNNNAF,
  title        = {Complex-Valued Neural Networks With Nonparametric Activation Functions},
  author       = {
    Scardapane, Simone and Van Vaerenbergh, Steven and Hussain, Amir and
    Uncini, Aurelio
  },
  year         = 2020,
  month        = {April},
  journal      = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume       = 4,
  number       = 2,
  pages        = {140--150},
  doi          = {10.1109/TETCI.2018.2872600},
  issn         = {2471-285X},
  abstract     = {
    Complex-valued neural networks (CVNNs) are a powerful modeling tool for
    domains where data can be naturally interpreted in terms of complex
    numbers. However, several analytical properties of the complex domain (such
    as holomorphicity) make the design of CVNNs a more challenging task than
    their real counterpart. In this paper, we consider the problem of flexible
    activation functions (AFs) in the complex domain, i.e., AFs endowed with
    sufficient degrees of freedom to adapt their shape given the training data.
    While this problem has received considerable attention in the real case,
    very limited literature exists for CVNNs, where most activation functions
    are generally developed in a split fashion (i.e., by considering the real
    and imaginary parts of the activation separately) or with simple
    phase-amplitude techniques. Leveraging over the recently proposed kernel
    activation functions, and related advances in the design of complex-valued
    kernels, we propose the first fully complex, nonparametric activation
    function for CVNNs, which is based on a kernel expansion with a fixed
    dictionary that can be implemented efficiently on vectorized hardware.
    Several experiments on common use cases, including prediction and channel
    equalization, validate our proposal when compared to real-valued neural
    networks and CVNNs with fixed activation functions.
  }
}
@inproceedings{Wang2020EncodingWorderOrder,
  title        = {Encoding word order in complex embeddings},
  author       = {
    Wang, Benyou and Zhao, Donghao and Lioma, Christina and Li, Qiuchi and
    Zhang, Peng and Simonsen, Jakob Grue
  },
  year         = 2020,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=Hke-WTVtwr}
}
@inproceedings{Islam2020PosENet,
  title        = {How much Position Information Do Convolutional Neural Networks Encode?},
  author       = {Islam, Md Amirul and Jia, Sen and Bruce, Neil D. B.},
  year         = 2020,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=rJeB36NKvB}
}
@inproceedings{Liu2020FLOATER,
  title        = {
    Learning to Encode Position for Transformer with Continuous Dynamical Model
  },
  author       = {Liu, Xuanqing and Yu, Hsiang-Fu and Dhillon, Inderjit and Hsieh, Cho-Jui},
  year         = 2020,
  month        = {13--18 Jul},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 119,
  pages        = {6327--6335},
  url          = {https://proceedings.mlr.press/v119/liu20n.html},
  editor       = {III, Hal Daumé and Singh, Aarti},
  pdf          = {http://proceedings.mlr.press/v119/liu20n/liu20n.pdf},
  abstract     = {
    We introduce a new way of learning to encode position information for
    non-recurrent models, such as Transformer models. Unlike RNN and LSTM,
    which contain inductive bias by loading the input tokens sequentially,
    non-recurrent models are less sensitive to position. The main reason is
    that position information among input units is not encoded inherently,
    i.e., they are permutation equivalent, this problem justifies why all of
    the existing models are accompanied by position encoding/embedding layer at
    the input. However, this solution has clear limitations: the sinusoidal
    position encoding is not flexible enough as it is manually designed and
    does not contain any learnable parameters, whereas the position embedding
    restricts the maximum length of input sequences. It is thus desirable to
    design a new position layer that contains learnable parameters to adjust to
    different datasets and different architectures. At the same time, we would
    also like it to extrapolate in accordance with the variable length of
    inputs. In our proposed solution, we borrow from the recent Neural ODE
    approach, which may be viewed as a versatile continuous version of a
    ResNet. This model is capable of modeling many kinds of dynamical systems.
    We model the evolution of encoded results along position index by such a
    dynamical system, thereby overcoming the above limitations of existing
    methods. We evaluate our new position layers on a variety of neural machine
    translation and language understanding tasks, the experimental results show
    consistent improvements over the baselines.
  }
}
@inproceedings{SemihKayhan2020ASLConv,
  title        = {
    On Translation Invariance in CNNs: Convolutional Layers Can Exploit
    Absolute Spatial Location
  },
  author       = {Semih Kayhan, Osman and van Gemert, Jan C.},
  year         = 2020,
  month        = {June},
  booktitle    = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {14262--14273},
  doi          = {10.1109/CVPR42600.2020.01428},
  issn         = {2575-7075},
  abstract     = {
    In this paper we challenge the common assumption that convolutional layers
    in modern CNNs are translation invariant. We show that CNNs can and will
    exploit the absolute spatial location by learning filters that respond
    exclusively to particular absolute locations by exploiting image boundary
    effects. Because modern CNNs filters have a huge receptive field, these
    boundary effects operate even far from the image boundary, allowing the
    network to exploit absolute spatial location all over the image. We give a
    simple solution to remove spatial location encoding which improves
    translation invariance and thus gives a stronger visual inductive bias
    which particularly benefits small data sets. We broadly demonstrate these
    benefits on several architectures and various applications such as image
    classification, patch matching, and two video classification datasets.
  }
}
@inproceedings{Dai2019Transformer-XL,
  title        = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
  author       = {
    Dai, Zihang  and Yang, Zhilin  and Yang, Yiming  and Carbonell, Jaime  and
    Le, Quoc  and Salakhutdinov, Ruslan
  },
  year         = 2019,
  month        = jul,
  booktitle    = {
    Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics
  },
  publisher    = {Association for Computational Linguistics},
  address      = {Florence, Italy},
  pages        = {2978--2988},
  doi          = {10.18653/v1/P19-1285},
  url          = {https://aclanthology.org/P19-1285},
  abstract     = {
    Transformers have a potential of learning longer-term dependency, but are
    limited by a fixed-length context in the setting of language modeling. We
    propose a novel neural architecture Transformer-XL that enables learning
    dependency beyond a fixed length without disrupting temporal coherence. It
    consists of a segment-level recurrence mechanism and a novel positional
    encoding scheme. Our method not only enables capturing longer-term
    dependency, but also resolves the context fragmentation problem. As a
    result, Transformer-XL learns dependency that is 80{\%} longer than RNNs
    and 450{\%} longer than vanilla Transformers, achieves better performance
    on both short and long sequences, and is up to 1,800+ times faster than
    vanilla Transformers during evaluation. Notably, we improve the
    state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on
    text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn
    Treebank (without finetuning). When trained only on WikiText-103,
    Transformer-XL manages to generate reasonably coherent, novel text articles
    with thousands of tokens. Our code, pretrained models, and hyperparameters
    are available in both Tensorflow and PyTorch.
  }
}
@inproceedings{Li2019CNM,
  title        = {CNM: An Interpretable Complex-valued Network for Matching},
  author       = {Li, Qiuchi  and Wang, Benyou  and Melucci, Massimo},
  year         = 2019,
  month        = jun,
  booktitle    = {
    Proceedings of the 2019 Conference of the North {A}merican Chapter of the
    Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)
  },
  publisher    = {Association for Computational Linguistics},
  address      = {Minneapolis, Minnesota},
  pages        = {4139--4148},
  doi          = {10.18653/v1/N19-1420},
  url          = {https://aclanthology.org/N19-1420},
  abstract     = {
    This paper seeks to model human language by the mathematical framework of
    quantum physics. With the well-designed mathematical formulations in
    quantum physics, this framework unifies different linguistic units in a
    single complex-valued vector space, e.g. words as particles in quantum
    states and sentences as mixed systems. A complex-valued network is built to
    implement this framework for semantic matching. With well-constrained
    complex-valued components, the network admits interpretations to explicit
    physical meanings. The proposed complex-valued network for matching (CNM)
    achieves comparable performances to strong CNN and RNN baselines on two
    benchmarking question answering (QA) datasets.
  }
}
@inproceedings{Loshchilov2018AdamW,
  title        = {Decoupled Weight Decay Regularization},
  author       = {Loshchilov, Ilya and Hutter, Frank},
  year         = 2019,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=Bkg6RiCqY7}
}
@inproceedings{Wang2019QPDN,
  title        = {Semantic Hilbert Space for Text Representation Learning},
  author       = {Wang, Benyou and Li, Qiuchi and Melucci, Massimo and Song, Dawei},
  year         = 2019,
  booktitle    = {The World Wide Web Conference},
  location     = {San Francisco, CA, USA},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {WWW '19},
  pages        = {3293–3299},
  doi          = {10.1145/3308558.3313516},
  isbn         = 9781450366748,
  url          = {https://doi.org/10.1145/3308558.3313516},
  abstract     = {
    Capturing the meaning of sentences has long been a challenging task.
    Current models tend to apply linear combinations of word features to
    conduct semantic composition for bigger-granularity units e.g. phrases,
    sentences, and documents. However, the semantic linearity does not always
    hold in human language. For instance, the meaning of the phrase “ivory
    tower” cannot be deduced by linearly combining the meanings of “ivory” and
    “tower”. To address this issue, we propose a new framework that models
    different levels of semantic units (e.g. sememe, word, sentence, and
    semantic abstraction) on a single Semantic Hilbert Space, which naturally
    admits a non-linear semantic composition by means of a complex-valued
    vector word representation. An end-to-end neural network 1 is proposed to
    implement the framework in the text classification task, and evaluation
    results on six benchmarking text classification datasets demonstrate the
    effectiveness, robustness and self-explanation power of the proposed model.
    Furthermore, intuitive case studies are conducted to help end users to
    understand how the framework works.
  },
  numpages     = 7,
  keywords     = {text understanding, neural network, quantum theory}
}
@inproceedings{Chen2018GradNorm,
  title        = {
    {G}rad{N}orm: Gradient Normalization for Adaptive Loss Balancing in Deep
    Multitask Networks
  },
  author       = {
    Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich,
    Andrew
  },
  year         = 2018,
  month        = {10--15 Jul},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {794--803},
  url          = {https://proceedings.mlr.press/v80/chen18a.html},
  editor       = {Dy, Jennifer and Krause, Andreas},
  pdf          = {http://proceedings.mlr.press/v80/chen18a/chen18a.pdf},
  abstract     = {
    Deep multitask networks, in which one neural network produces multiple
    predictive outputs, can offer better speed and performance than their
    single-task counterparts but are challenging to train properly. We present
    a gradient normalization (GradNorm) algorithm that automatically balances
    training in deep multitask models by dynamically tuning gradient
    magnitudes. We show that for various network architectures, for both
    regression and classification tasks, and on both synthetic and real
    datasets, GradNorm improves accuracy and reduces overfitting across
    multiple tasks when compared to single-task networks, static baselines, and
    other adaptive multitask loss balancing techniques. GradNorm also matches
    or surpasses the performance of exhaustive grid search methods, despite
    only involving a single asymmetry hyperparameter $\alpha$. Thus, what was
    once a tedious search process that incurred exponentially more compute for
    each task added can now be accomplished within a few training runs,
    irrespective of the number of tasks. Ultimately, we will demonstrate that
    gradient manipulation affords us great control over the training dynamics
    of multitask networks and may be one of the keys to unlocking the potential
    of multitask learning.
  }
}
@inproceedings{Parcollet2018QCNN,
  title        = {
    {Quaternion Convolutional Neural Networks for End-to-End Automatic Speech
    Recognition}
  },
  author       = {
    Parcollet, Titouan and Zhang, Ying and Morchid, Mohamed and Trabelsi,
    Chiheb and Linares, Georges and {de Mori}, Renato and Bengio, Yoshua
  },
  year         = 2018,
  booktitle    = {Proc. Interspeech 2018},
  pages        = {22--26},
  doi          = {10.21437/Interspeech.2018-1898}
}
@inproceedings{Trabelsi2018DCN,
  title        = {Deep Complex Networks},
  author       = {
    Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy
    and Subramanian, Sandeep and Santos, Joao Felipe and Mehri, Soroush and
    Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J
  },
  year         = 2018,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=H1T2hmZAb}
}
@article{Agarap2018ReLU,
  title        = {Deep learning using rectified linear units (relu)},
  author       = {Agarap, Abien Fred},
  year         = 2018,
  journal      = {arXiv preprint arXiv:1803.08375}
}
@inproceedings{Jacot2018NTK,
  title        = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author       = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
  year         = 2018,
  booktitle    = {
    Proceedings of the 32nd International Conference on Neural Information
    Processing Systems
  },
  location     = {Montr\'{e}al, Canada},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NIPS'18},
  pages        = {8580–8589},
  abstract     = {
    At initialization, artificial neural networks (ANNs) are equivalent to
    Gaussian processes in the infinite-width limit [12, 9], thus connecting
    them to kernel methods. We prove that the evolution of an ANN during
    training can also be described by a kernel: during gradient descent on the
    parameters of an ANN, the network function fθ (which maps input vectors to
    output vectors) follows the kernel gradient of the functional cost (which
    is convex, in contrast to the parameter cost) w.r.t. a new kernel: the
    Neural Tangent Kernel (NTK). This kernel is central to describe the
    generalization features of ANNs. While the NTK is random at initialization
    and varies during training, in the infinite-width limit it converges to an
    explicit limiting kernel and it stays constant during training. This makes
    it possible to study the training of ANNs in function space instead of
    parameter space. Convergence of the training can then be related to the
    positive-definiteness of the limiting NTK. We then focus on the setting of
    least-squares regression and show that in the infinite-width limit, the
    network function fθ follows a linear differential equation during training.
    The convergence is fastest along the largest kernel principal components of
    the input data with respect to the NTK, hence suggesting a theoretical
    motivation for early stopping.Finally we study the NTK numerically, observe
    its behavior for wide networks, and compare it to the infinite-width limit.
  },
  numpages     = 10
}
@inproceedings{Shaw2018RPE,
  title        = {Self-Attention with Relative Position Representations},
  author       = {Shaw, Peter  and Uszkoreit, Jakob  and Vaswani, Ashish},
  year         = 2018,
  month        = jun,
  booktitle    = {
    Proceedings of the 2018 Conference of the North {A}merican Chapter of the
    Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)
  },
  publisher    = {Association for Computational Linguistics},
  address      = {New Orleans, Louisiana},
  pages        = {464--468},
  doi          = {10.18653/v1/N18-2074},
  url          = {https://aclanthology.org/N18-2074},
  abstract     = {
    Relying entirely on an attention mechanism, the Transformer introduced by
    Vaswani et al. (2017) achieves state-of-the-art results for machine
    translation. In contrast to recurrent and convolutional neural networks, it
    does not explicitly model relative or absolute position information in its
    structure. Instead, it requires adding representations of absolute
    positions to its inputs. In this work we present an alternative approach,
    extending the self-attention mechanism to efficiently consider
    representations of the relative positions, or distances between sequence
    elements. On the WMT 2014 English-to-German and English-to-French
    translation tasks, this approach yields improvements of 1.3 BLEU and 0.3
    BLEU over absolute position representations, respectively. Notably, we
    observe that combining relative and absolute position representations
    yields no further improvement in translation quality. We describe an
    efficient implementation of our method and cast it as an instance of
    relation-aware self-attention mechanisms that can generalize to arbitrary
    graph-labeled inputs.
  }
}
@inproceedings{Gehring2017APE,
  title        = {Convolutional Sequence to Sequence Learning},
  author       = {
    Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and
    Dauphin, Yann N.
  },
  year         = 2017,
  month        = {06--11 Aug},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 70,
  pages        = {1243--1252},
  url          = {https://proceedings.mlr.press/v70/gehring17a.html},
  editor       = {Precup, Doina and Teh, Yee Whye},
  pdf          = {http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf},
  abstract     = {
    The prevalent approach to sequence to sequence learning maps an input
    sequence to a variable length output sequence via recurrent neural
    networks. We introduce an architecture based entirely on convolutional
    neural networks. Compared to recurrent models, computations over all
    elements can be fully parallelized during training to better exploit the
    GPU hardware and optimization is easier since the number of non-linearities
    is fixed and independent of the input length. Our use of gated linear units
    eases gradient propagation and we equip each decoder layer with a separate
    attention module. We outperform the accuracy of the deep LSTM setup of Wu
    et al. (2016) on both WMT’14 English-German and WMT’14 English-French
    translation at an order of magnitude faster speed, both on GPU and CPU.
  }
}
@inproceedings{Vaswani2017Transformer,
  title        = {Attention is All you Need},
  author       = {
    Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and
    Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia
  },
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 30,
  url          = {
    https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
  },
  editor       = {
    I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and
    S. Vishwanathan and R. Garnett
  }
}
@inproceedings{Trouillon2016ComplEx,
  title        = {Complex Embeddings for Simple Link Prediction},
  author       = {
    Trouillon, Théo and Welbl, Johannes and Riedel, Sebastian and Gaussier,
    Eric and Bouchard, Guillaume
  },
  year         = 2016,
  month        = {20--22 Jun},
  booktitle    = {Proceedings of The 33rd International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {New York, New York, USA},
  series       = {Proceedings of Machine Learning Research},
  volume       = 48,
  pages        = {2071--2080},
  url          = {https://proceedings.mlr.press/v48/trouillon16.html},
  editor       = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  pdf          = {http://proceedings.mlr.press/v48/trouillon16.pdf},
  abstract     = {
    In statistical relational learning, the link prediction problem is key to
    automatically understand the structure of large knowledge bases. As in
    previous studies, we propose to solve this problem through latent
    factorization. However, here we make use of complex valued embeddings. The
    composition of complex embeddings can handle a large variety of binary
    relations, among them symmetric and antisymmetric relations. Compared to
    state-of-the-art models such as Neural Tensor Network and Holographic
    Embeddings, our approach based on complex embeddings is arguably simpler,
    as it only uses the Hermitian dot product, the complex counterpart of the
    standard dot product between real vectors. Our approach is scalable to
    large datasets as it remains linear in both space and time, while
    consistently outperforming alternative approaches on standard link
    prediction benchmarks.
  }
}
@inproceedings{Wisdom2016uRNN,
  title        = {Full-Capacity Unitary Recurrent Neural Networks},
  author       = {
    Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan
    and Atlas, Les
  },
  year         = 2016,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 29,
  url          = {
    https://proceedings.neurips.cc/paper_files/paper/2016/file/d9ff90f4000eacd3a6c9cb27f78994cf-Paper.pdf
  },
  editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett}
}
@article{Ba2016LN,
  title        = {Layer normalization},
  author       = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  year         = 2016,
  journal      = {arXiv preprint arXiv:1607.06450}
}
@article{Hendrycks2016GELU,
  title        = {Gaussian error linear units (gelus)},
  author       = {Hendrycks, Dan and Gimpel, Kevin},
  year         = 2016,
  journal      = {arXiv preprint arXiv:1606.08415}
}
@inproceedings{Popa2016OVNN,
  title        = {Octonion-Valued Neural Networks},
  author       = {Popa, C{\u{a}}lin-Adrian},
  year         = 2016,
  booktitle    = {Artificial Neural Networks and Machine Learning -- ICANN 2016},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  pages        = {435--443},
  isbn         = {978-3-319-44778-0},
  editor       = {Villa, Alessandro E.P. and Masulli, Paolo and Pons Rivero, Antonio Javier},
  abstract     = {
    Neural networks with values in multidimensional domains have been
    intensively studied over the last few years. This paper introduces
    octonion-valued neural networks, for which the inputs, outputs, weights and
    biases are all octonions. They represent a generalization of the complex-
    and quaternion-valued neural networks, that do not fall into the category
    of Clifford-valued neural networks, because, unlike Clifford algebras, the
    octonion algebra is not associative. The full deduction of the gradient
    descent algorithm for training octonion-valued feedforward neural networks
    is presented. Testing of the proposed network is done using two synthetic
    function approximation problems and a time series prediction application.
  }
}
@inproceedings{Jaderberg2015STN,
  title        = {Spatial Transformer Networks},
  author       = {
    Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and kavukcuoglu,
    koray
  },
  year         = 2015,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 28,
  url          = {
    https://proceedings.neurips.cc/paper_files/paper/2015/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf
  },
  editor       = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett}
}
@inproceedings{Ioffe2015BN,
  title        = {
    Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift
  },
  author       = {Ioffe, Sergey and Szegedy, Christian},
  year         = 2015,
  month        = {07--09 Jul},
  booktitle    = {Proceedings of the 32nd International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {Lille, France},
  series       = {Proceedings of Machine Learning Research},
  volume       = 37,
  pages        = {448--456},
  url          = {https://proceedings.mlr.press/v37/ioffe15.html},
  editor       = {Bach, Francis and Blei, David},
  pdf          = {http://proceedings.mlr.press/v37/ioffe15.pdf},
  abstract     = {
    Training Deep Neural Networks is complicated by the fact that the
    distribution of each layer’s inputs changes during training, as the
    parameters of the previous layers change. This slows down the training by
    requiring lower learning rates and careful parameter initialization, and
    makes it notoriously hard to train models with saturating nonlinearities.
    We refer to this phenomenon as internal covariate shift, and address the
    problem by normalizing layer inputs. Our method draws its strength from
    making normalization a part of the model architecture and performing the
    normalization for each training mini-batch. Batch Normalization allows us
    to use much higher learning rates and be less careful about initialization,
    and in some cases eliminates the need for Dropout. Applied to a
    stateof-the-art image classification model, Batch Normalization achieves
    the same accuracy with 14 times fewer training steps, and beats the
    original model by a significant margin. Using an ensemble of
    batch-normalized networks, we improve upon the best published result on
    ImageNet classification: reaching 4.82% top-5 test error, exceeding the
    accuracy of human raters.
  }
}
@article{Reichert2013Neuronal,
  title        = {Neuronal Synchrony in Complex-Valued Deep Networks},
  author       = {Reichert, David P. and Serre, Thomas},
  year         = 2013,
  journal      = {International Conference On Learning Representations},
  bibsource    = {
    Semantic Scholar
    https://www.semanticscholar.org/paper/7c5920c97f8bb1f91739b0d27746d655de95eedd
  }
}
@inproceedings{Sutskever2013SGD,
  title        = {On the importance of initialization and momentum in deep learning},
  author       = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year         = 2013,
  month        = {17--19 Jun},
  booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {Atlanta, Georgia, USA},
  series       = {Proceedings of Machine Learning Research},
  volume       = 28,
  number       = 3,
  pages        = {1139--1147},
  url          = {https://proceedings.mlr.press/v28/sutskever13.html},
  editor       = {Dasgupta, Sanjoy and McAllester, David},
  pdf          = {http://proceedings.mlr.press/v28/sutskever13.pdf},
  abstract     = {
    Deep and recurrent neural networks (DNNs and RNNs respectively) are
    powerful models that were considered to be almost impossible to train using
    stochastic gradient descent with momentum. In this paper, we show that when
    stochastic gradient descent with momentum uses a well-designed random
    initialization and a particular type of slowly increasing schedule for the
    momentum parameter, it can train both DNNs and RNNs (on datasets with
    long-term dependencies) to levels of performance that were previously
    achievable only with Hessian-Free optimization. We find that both the
    initialization and the momentum are crucial since poorly initialized
    networks cannot be trained with momentum and well-initialized networks
    perform markedly worse when the momentum is absent or poorly tuned.     Our
    success training these models suggests that previous attempts to train deep
    and recurrent neural networks from random initializations have likely
    failed due to poor initialization schemes. Furthermore, carefully tuned
    momentum methods suffice for dealing with the curvature issues in deep and
    recurrent network training objectives without the need for sophisticated
    second-order methods.
  }
}
@article{Buchholz2008CliffordNN,
  title        = {On Clifford neurons and Clifford multi-layer perceptrons},
  author       = {Buchholz, Sven and Sommer, Gerald},
  year         = 2008,
  journal      = {Neural Networks},
  volume       = 21,
  number       = 7,
  pages        = {925--935},
  doi          = {https://doi.org/10.1016/j.neunet.2008.03.004},
  issn         = {0893-6080},
  url          = {https://www.sciencedirect.com/science/article/pii/S0893608008000592},
  keywords     = {
    Clifford (geometric) algebra, Clifford neural networks, Clifford neurons,
    Multi-layer perceptrons, Backpropagation, Function approximation
  },
  abstract     = {
    We study the framework of Clifford algebra for the design of neural
    architectures capable of processing different geometric entities. The
    benefits of this model-based computation over standard real-valued networks
    are demonstrated. One particular example thereof is the new class of
    so-called Spinor Clifford neurons. The paper provides a sound theoretical
    basis to Clifford neural computation. For that purpose the new concepts of
    isomorphic neurons and isomorphic representations are introduced. A unified
    training rule for Clifford MLPs is also provided. The topic of activation
    functions for Clifford MLPs is discussed in detail for all two-dimensional
    Clifford algebras for the first time.
  }
}
@article{Nitta2004DecisionBoundary,
  title        = {{Orthogonality of Decision Boundaries in Complex-Valued Neural Networks}},
  author       = {Nitta, Tohru},
  year         = 2004,
  month        = {01},
  journal      = {Neural Computation},
  volume       = 16,
  number       = 1,
  pages        = {73--97},
  doi          = {10.1162/08997660460734001},
  issn         = {0899-7667},
  url          = {https://doi.org/10.1162/08997660460734001},
  abstract     = {
    {This letter presents some results of an analysis on the decision
    boundaries of complex-valued neural networks whose weights, threshold
    values, input and output signals are all complex numbers. The main results
    may be summarized as follows. (1) A decision boundary of a single
    complex-valued neuron consists of two hypersurfaces that intersect
    orthogonally, and divides a decision region into four equal sections. The
    XOR problem and the detection of symmetry problem that cannot be solved
    with two-layered real-valued neural networks, can be solved by two-layered
    complex-valued neural networks with the orthogonal decision boundaries,
    which reveals a potent computational power of complex-valued neural nets.
    Furthermore, the fading equalization problem can be successfully solved by
    the two-layered complex-valued neural network with the highest
    generalization ability. (2) A decision boundary of a three-layered
    complex-valued neural network has the orthogonal property as a basic
    structure, and its two hypersurfaces approach orthogonality as all the net
    inputs to each hidden neuron grow. In particular, most of the decision
    boundaries in the three-layered complex-valued neural network inetersect
    orthogonally when the network is trained using Complex-BP algorithm. As a
    result, the orthogonality of the decision boundaries improves its
    generalization ability. (3) The average of the learning speed of the
    Complex-BP is several times faster than that of the Real-BP. The standard
    deviation of the learning speed of the Complex-BP is smaller than that of
    the Real-BP. It seems that the complex-valued neural network and the
    related algorithm are natural for learning complex-valued patterns for the
    above reasons.}
  },
  eprint       = {
    https://direct.mit.edu/neco/article-pdf/16/1/73/815724/08997660460734001.pdf
  }
}
@inproceedings{Isokawa2003QNN,
  title        = {Quaternion Neural Network and Its Application},
  author       = {
    Isokawa, Teijiro and Kusakabe, Tomoaki and Matsui, Nobuyuki and Peper,
    Ferdinand
  },
  year         = 2003,
  booktitle    = {Knowledge-Based Intelligent Information and Engineering Systems},
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  pages        = {318--324},
  isbn         = {978-3-540-45226-3},
  editor       = {Palade, Vasile and Howlett, Robert J. and Jain, Lakhmi},
  abstract     = {
    Quaternion neural networks are models of which computations in the neurons
    is based on quaternions, the four-dimensional equivalents of imaginary
    numbers. This paper shows by experiments that the quaternion-version of the
    Back Propagation (BP) algorithm achieves correct geometrical
    transformations in color space for an image compression problem, whereas
    real-valued BP algorithms fail.
  }
}
@article{Goto1954Parametron,
  title        = {The parametron--A new circuit element which utilizes non-linear reactors},
  author       = {Goto, Eiichi},
  year         = 1954,
  journal      = {
    Paper of Technical Group of Electronic Computers and Nonlinear Theory, IECE
    (July 1954)(in Japanese)
  }
}
@article{Wirtinger1927Calculus,
  title        = {Zur formalen Theorie der Funktionen von mehr komplexen Ver{\"a}nderlichen},
  author       = {Wirtinger, W.},
  year         = 1927,
  month        = {Dec},
  day          = {01},
  journal      = {Mathematische Annalen},
  volume       = 97,
  number       = 1,
  pages        = {357--375},
  doi          = {10.1007/BF01447872},
  issn         = {1432-1807},
  url          = {https://doi.org/10.1007/BF01447872}
}
