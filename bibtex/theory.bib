@inproceedings{Dosovitskiy2021ViT,
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author       = {
    Dosovitskiy, lexey and Beyer, Lucas and Kolesnikov, Alexander and
    Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani,
    Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and
    Uszkoreit, Jakob and Houlsby, Neil
  },
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=YicbFdNTTy}
}
@article{Scardapane2020CVNNNAF,
  title        = {Complex-Valued Neural Networks With Nonparametric Activation Functions},
  author       = {
    Scardapane, Simone and Van Vaerenbergh, Steven and Hussain, Amir and
    Uncini, Aurelio
  },
  year         = 2020,
  month        = {April},
  journal      = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume       = 4,
  number       = 2,
  pages        = {140--150},
  doi          = {10.1109/TETCI.2018.2872600},
  issn         = {2471-285X},
  abstract     = {
    Complex-valued neural networks (CVNNs) are a powerful modeling tool for
    domains where data can be naturally interpreted in terms of complex
    numbers. However, several analytical properties of the complex domain (such
    as holomorphicity) make the design of CVNNs a more challenging task than
    their real counterpart. In this paper, we consider the problem of flexible
    activation functions (AFs) in the complex domain, i.e., AFs endowed with
    sufficient degrees of freedom to adapt their shape given the training data.
    While this problem has received considerable attention in the real case,
    very limited literature exists for CVNNs, where most activation functions
    are generally developed in a split fashion (i.e., by considering the real
    and imaginary parts of the activation separately) or with simple
    phase-amplitude techniques. Leveraging over the recently proposed kernel
    activation functions, and related advances in the design of complex-valued
    kernels, we propose the first fully complex, nonparametric activation
    function for CVNNs, which is based on a kernel expansion with a fixed
    dictionary that can be implemented efficiently on vectorized hardware.
    Several experiments on common use cases, including prediction and channel
    equalization, validate our proposal when compared to real-valued neural
    networks and CVNNs with fixed activation functions.
  }
}
@inproceedings{Islam2020PosENet,
  title        = {How much Position Information Do Convolutional Neural Networks Encode?},
  author       = {Islam, Md Amirul and Jia, Sen and Bruce, Neil D. B.},
  year         = 2020,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=rJeB36NKvB}
}
@inproceedings{Loshchilov2018AdamW,
  title        = {Decoupled Weight Decay Regularization},
  author       = {Loshchilov, Ilya and Hutter, Frank},
  year         = 2019,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=Bkg6RiCqY7}
}
@inproceedings{Chen2018GradNorm,
  title        = {
    {G}rad{N}orm: Gradient Normalization for Adaptive Loss Balancing in Deep
    Multitask Networks
  },
  author       = {
    Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich,
    Andrew
  },
  year         = 2018,
  month        = {10--15 Jul},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {794--803},
  url          = {https://proceedings.mlr.press/v80/chen18a.html},
  editor       = {Dy, Jennifer and Krause, Andreas},
  pdf          = {http://proceedings.mlr.press/v80/chen18a/chen18a.pdf},
  abstract     = {
    Deep multitask networks, in which one neural network produces multiple
    predictive outputs, can offer better speed and performance than their
    single-task counterparts but are challenging to train properly. We present
    a gradient normalization (GradNorm) algorithm that automatically balances
    training in deep multitask models by dynamically tuning gradient
    magnitudes. We show that for various network architectures, for both
    regression and classification tasks, and on both synthetic and real
    datasets, GradNorm improves accuracy and reduces overfitting across
    multiple tasks when compared to single-task networks, static baselines, and
    other adaptive multitask loss balancing techniques. GradNorm also matches
    or surpasses the performance of exhaustive grid search methods, despite
    only involving a single asymmetry hyperparameter $\alpha$. Thus, what was
    once a tedious search process that incurred exponentially more compute for
    each task added can now be accomplished within a few training runs,
    irrespective of the number of tasks. Ultimately, we will demonstrate that
    gradient manipulation affords us great control over the training dynamics
    of multitask networks and may be one of the keys to unlocking the potential
    of multitask learning.
  }
}
@inproceedings{Trabelsi2018DCN,
  title        = {Deep Complex Networks},
  author       = {
    Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy
    and Subramanian, Sandeep and Santos, Joao Felipe and Mehri, Soroush and
    Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J
  },
  year         = 2018,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=H1T2hmZAb}
}
@inproceedings{Jacot2018NTK,
  title        = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author       = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
  year         = 2018,
  booktitle    = {
    Proceedings of the 32nd International Conference on Neural Information
    Processing Systems
  },
  location     = {Montr\'{e}al, Canada},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NIPS'18},
  pages        = {8580–8589},
  abstract     = {
    At initialization, artificial neural networks (ANNs) are equivalent to
    Gaussian processes in the infinite-width limit [12, 9], thus connecting
    them to kernel methods. We prove that the evolution of an ANN during
    training can also be described by a kernel: during gradient descent on the
    parameters of an ANN, the network function fθ (which maps input vectors to
    output vectors) follows the kernel gradient of the functional cost (which
    is convex, in contrast to the parameter cost) w.r.t. a new kernel: the
    Neural Tangent Kernel (NTK). This kernel is central to describe the
    generalization features of ANNs. While the NTK is random at initialization
    and varies during training, in the infinite-width limit it converges to an
    explicit limiting kernel and it stays constant during training. This makes
    it possible to study the training of ANNs in function space instead of
    parameter space. Convergence of the training can then be related to the
    positive-definiteness of the limiting NTK. We then focus on the setting of
    least-squares regression and show that in the infinite-width limit, the
    network function fθ follows a linear differential equation during training.
    The convergence is fastest along the largest kernel principal components of
    the input data with respect to the NTK, hence suggesting a theoretical
    motivation for early stopping.Finally we study the NTK numerically, observe
    its behavior for wide networks, and compare it to the infinite-width limit.
  },
  numpages     = 10
}
@inproceedings{Vaswani2017Transformer,
  title        = {Attention is All you Need},
  author       = {
    Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and
    Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia
  },
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 30,
  url          = {
    https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
  },
  editor       = {
    I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and
    S. Vishwanathan and R. Garnett
  }
}
@inproceedings{Jaderberg2015STN,
  title        = {Spatial Transformer Networks},
  author       = {
    Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and kavukcuoglu,
    koray
  },
  year         = 2015,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 28,
  url          = {
    https://proceedings.neurips.cc/paper_files/paper/2015/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf
  },
  editor       = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett}
}
@inproceedings{Sutskever2013SGD,
  title        = {On the importance of initialization and momentum in deep learning},
  author       = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year         = 2013,
  month        = {17--19 Jun},
  booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {Atlanta, Georgia, USA},
  series       = {Proceedings of Machine Learning Research},
  volume       = 28,
  number       = 3,
  pages        = {1139--1147},
  url          = {https://proceedings.mlr.press/v28/sutskever13.html},
  editor       = {Dasgupta, Sanjoy and McAllester, David},
  pdf          = {http://proceedings.mlr.press/v28/sutskever13.pdf},
  abstract     = {
    Deep and recurrent neural networks (DNNs and RNNs respectively) are
    powerful models that were considered to be almost impossible to train using
    stochastic gradient descent with momentum. In this paper, we show that when
    stochastic gradient descent with momentum uses a well-designed random
    initialization and a particular type of slowly increasing schedule for the
    momentum parameter, it can train both DNNs and RNNs (on datasets with
    long-term dependencies) to levels of performance that were previously
    achievable only with Hessian-Free optimization. We find that both the
    initialization and the momentum are crucial since poorly initialized
    networks cannot be trained with momentum and well-initialized networks
    perform markedly worse when the momentum is absent or poorly tuned.     Our
    success training these models suggests that previous attempts to train deep
    and recurrent neural networks from random initializations have likely
    failed due to poor initialization schemes. Furthermore, carefully tuned
    momentum methods suffice for dealing with the curvature issues in deep and
    recurrent network training objectives without the need for sophisticated
    second-order methods.
  }
}
