@inproceedings{Chen2018GradNorm,
  title        = {
    {G}rad{N}orm: Gradient Normalization for Adaptive Loss Balancing in Deep
    Multitask Networks
  },
  author       = {
    Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich,
    Andrew
  },
  year         = 2018,
  month        = {10--15 Jul},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {794--803},
  url          = {https://proceedings.mlr.press/v80/chen18a.html},
  editor       = {Dy, Jennifer and Krause, Andreas},
  pdf          = {http://proceedings.mlr.press/v80/chen18a/chen18a.pdf},
  abstract     = {
    Deep multitask networks, in which one neural network produces multiple
    predictive outputs, can offer better speed and performance than their
    single-task counterparts but are challenging to train properly. We present
    a gradient normalization (GradNorm) algorithm that automatically balances
    training in deep multitask models by dynamically tuning gradient
    magnitudes. We show that for various network architectures, for both
    regression and classification tasks, and on both synthetic and real
    datasets, GradNorm improves accuracy and reduces overfitting across
    multiple tasks when compared to single-task networks, static baselines, and
    other adaptive multitask loss balancing techniques. GradNorm also matches
    or surpasses the performance of exhaustive grid search methods, despite
    only involving a single asymmetry hyperparameter $\alpha$. Thus, what was
    once a tedious search process that incurred exponentially more compute for
    each task added can now be accomplished within a few training runs,
    irrespective of the number of tasks. Ultimately, we will demonstrate that
    gradient manipulation affords us great control over the training dynamics
    of multitask networks and may be one of the keys to unlocking the potential
    of multitask learning.
  }
}
@inproceedings{Jacot2018NTK,
  title        = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author       = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
  year         = 2018,
  booktitle    = {
    Proceedings of the 32nd International Conference on Neural Information
    Processing Systems
  },
  location     = {Montr\'{e}al, Canada},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NIPS'18},
  pages        = {8580–8589},
  abstract     = {
    At initialization, artificial neural networks (ANNs) are equivalent to
    Gaussian processes in the infinite-width limit [12, 9], thus connecting
    them to kernel methods. We prove that the evolution of an ANN during
    training can also be described by a kernel: during gradient descent on the
    parameters of an ANN, the network function fθ (which maps input vectors to
    output vectors) follows the kernel gradient of the functional cost (which
    is convex, in contrast to the parameter cost) w.r.t. a new kernel: the
    Neural Tangent Kernel (NTK). This kernel is central to describe the
    generalization features of ANNs. While the NTK is random at initialization
    and varies during training, in the infinite-width limit it converges to an
    explicit limiting kernel and it stays constant during training. This makes
    it possible to study the training of ANNs in function space instead of
    parameter space. Convergence of the training can then be related to the
    positive-definiteness of the limiting NTK. We then focus on the setting of
    least-squares regression and show that in the infinite-width limit, the
    network function fθ follows a linear differential equation during training.
    The convergence is fastest along the largest kernel principal components of
    the input data with respect to the NTK, hence suggesting a theoretical
    motivation for early stopping.Finally we study the NTK numerically, observe
    its behavior for wide networks, and compare it to the infinite-width limit.
  },
  numpages     = 10
}
