@inproceedings{Ding2022LargeKernelDesign,
  title        = {Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs},
  author       = {Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
  year         = 2022,
  month        = {June},
  booktitle    = {
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)
  },
  pages        = {11963--11975}
}
@inproceedings{Wang2021PE-BERT,
  title        = {On Position Embeddings in BERT},
  author       = {
    Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang
    and Qun Liu and Jakob Grue Simonsen
  },
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=onxoVA9FxMw}
}
@inproceedings{Alsallakh2021MindThePad,
  title        = {Mind the Pad -- CNNs Can Develop Blind Spots},
  author       = {
    Bilal Alsallakh and Narine Kokhlikyan and Vivek Miglani and Jun Yuan and
    Orion Reblitz-Richardson
  },
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=m1CD7tPubNy}
}
@inproceedings{Dosovitskiy2021ViT,
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author       = {
    Dosovitskiy, lexey and Beyer, Lucas and Kolesnikov, Alexander and
    Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani,
    Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and
    Uszkoreit, Jakob and Houlsby, Neil
  },
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=YicbFdNTTy}
}
@article{Islam2021Position,
  title        = {
    Position, padding and predictions: A deeper look at position information in
    cnns
  },
  author       = {
    Islam, Md Amirul and Kowal, Matthew and Jia, Sen and Derpanis, Konstantinos
    G and Bruce, Neil DB
  },
  year         = 2021,
  journal      = {arXiv preprint arXiv:2101.12322}
}
@inproceedings{Narang2021Transformer,
  title        = {
    Do Transformer Modifications Transfer Across Implementations and
    Applications?
  },
  author       = {
    Narang, Sharan  and Chung, Hyung Won  and Tay, Yi  and Fedus, Liam  and
    Fevry, Thibault  and Matena, Michael  and Malkan, Karishma  and Fiedel,
    Noah  and Shazeer, Noam  and Lan, Zhenzhong  and Zhou, Yanqi  and Li, Wei
    and Ding, Nan  and Marcus, Jake  and Roberts, Adam  and Raffel, Colin
  },
  year         = 2021,
  month        = nov,
  booktitle    = {
    Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing
  },
  publisher    = {Association for Computational Linguistics},
  address      = {Online and Punta Cana, Dominican Republic},
  pages        = {5758--5773},
  doi          = {10.18653/v1/2021.emnlp-main.465},
  url          = {https://aclanthology.org/2021.emnlp-main.465},
  abstract     = {
    The research community has proposed copious modifications to the
    Transformer architecture since it was introduced over three years ago,
    relatively few of which have seen widespread adoption. In this paper, we
    comprehensively evaluate many of these modifications in a shared
    experimental setting that covers most of the common uses of the Transformer
    in natural language processing. Surprisingly, we find that most
    modifications do not meaningfully improve performance. Furthermore, most of
    the Transformer variants we found beneficial were either developed in the
    same codebase that we used or are relatively minor changes. We conjecture
    that performance improvements may strongly depend on implementation details
    and correspondingly make some recommendations for improving the generality
    of experimental results.
  }
}
@inproceedings{Wu2021RPE,
  title        = {Rethinking and Improving Relative Position Encoding for Vision Transformer},
  author       = {
    Wu, Kan and Peng, Houwen and Chen, Minghao and Fu, Jianlong and Chao,
    Hongyang
  },
  year         = 2021,
  month        = {Oct},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages        = {10013--10021},
  doi          = {10.1109/ICCV48922.2021.00988},
  issn         = {2380-7504},
  abstract     = {
    Relative position encoding (RPE) is important for transformer to capture
    sequence ordering of input tokens. General efficacy has been proven in
    natural language processing. However, in computer vision, its efficacy is
    not well studied and even remains controversial, e.g., whether relative
    position encoding can work equally well as absolute position? In order to
    clarify this, we first review existing relative position encoding methods
    and analyze their pros and cons when applied in vision transformers. We
    then propose new relative position encoding methods dedicated to 2D images,
    called image RPE (iRPE). Our methods consider directional relative distance
    modeling as well as the interactions between queries and relative position
    embeddings in self-attention mechanism. The proposed iRPE methods are
    simple and lightweight. They can be easily plugged into transformer blocks.
    Experiments demonstrate that solely due to the proposed encoding methods,
    DeiT [21] and DETR [1] obtain up to 1.5% (top-1 Acc) and 1.3% (mAP) stable
    improvements over their original versions on ImageNet and COCO
    respectively, without tuning any extra hyperparameters such as learning
    rate and weight decay. Our ablation and analysis also yield interesting
    findings, some of which run counter to previous understanding. Code and
    models are open-sourced at
    https://github.com/microsoft/Cream/tree/main/iRPE.
  }
}
@inproceedings{Xu2021PEinGANs,
  title        = {Positional Encoding as Spatial Inductive Bias in GANs},
  author       = {
    Xu, Rui and Wang, Xintao and Chen, Kai and Zhou, Bolei and Loy, Chen Change
  },
  year         = 2021,
  month        = {June},
  booktitle    = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {13564--13573},
  doi          = {10.1109/CVPR46437.2021.01336},
  issn         = {2575-7075},
  abstract     = {
    SinGAN shows impressive capability in learning internal patch distribution
    despite its limited effective receptive field. We are interested in knowing
    how such a translationinvariant convolutional generator could capture the
    global structure with just a spatially i.i.d. input. In this work, taking
    SinGAN and StyleGAN2 as examples, we show that such capability, to a large
    extent, is brought by the implicit positional encoding when using zero
    padding in the generators. Such positional encoding is indispensable for
    generating images with high fidelity. The same phenomenon is observed in
    other generative architectures such as DCGAN and PGGAN. We further show
    that zero padding leads to an unbalanced spatial bias with a vague relation
    between locations. To offer a better spatial inductive bias, we investigate
    alternative positional encodings and analyze their effects. Based on a more
    flexible positional encoding explicitly, we propose a new multi-scale
    training strategy and demonstrate its effectiveness in the state-of-the-art
    unconditional generator StyleGAN2. Besides, the explicit spatial inductive
    bias substantially improves SinGAN for more versatile image manipulation. 1
  }
}
@inproceedings{He2021DEBERTA,
  title        = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author       = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=XPZIaotutsD}
}
@article{Raffel2020T5,
  title        = {
    Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer
  },
  author       = {
    Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and
    Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu,
    Peter J.
  },
  year         = 2020,
  journal      = {Journal of Machine Learning Research},
  volume       = 21,
  number       = 140,
  pages        = {1--67},
  url          = {http://jmlr.org/papers/v21/20-074.html}
}
@article{Scardapane2020CVNNNAF,
  title        = {Complex-Valued Neural Networks With Nonparametric Activation Functions},
  author       = {
    Scardapane, Simone and Van Vaerenbergh, Steven and Hussain, Amir and
    Uncini, Aurelio
  },
  year         = 2020,
  month        = {April},
  journal      = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume       = 4,
  number       = 2,
  pages        = {140--150},
  doi          = {10.1109/TETCI.2018.2872600},
  issn         = {2471-285X},
  abstract     = {
    Complex-valued neural networks (CVNNs) are a powerful modeling tool for
    domains where data can be naturally interpreted in terms of complex
    numbers. However, several analytical properties of the complex domain (such
    as holomorphicity) make the design of CVNNs a more challenging task than
    their real counterpart. In this paper, we consider the problem of flexible
    activation functions (AFs) in the complex domain, i.e., AFs endowed with
    sufficient degrees of freedom to adapt their shape given the training data.
    While this problem has received considerable attention in the real case,
    very limited literature exists for CVNNs, where most activation functions
    are generally developed in a split fashion (i.e., by considering the real
    and imaginary parts of the activation separately) or with simple
    phase-amplitude techniques. Leveraging over the recently proposed kernel
    activation functions, and related advances in the design of complex-valued
    kernels, we propose the first fully complex, nonparametric activation
    function for CVNNs, which is based on a kernel expansion with a fixed
    dictionary that can be implemented efficiently on vectorized hardware.
    Several experiments on common use cases, including prediction and channel
    equalization, validate our proposal when compared to real-valued neural
    networks and CVNNs with fixed activation functions.
  }
}
@inproceedings{Wang2020EncodingWorderOrder,
  title        = {Encoding word order in complex embeddings},
  author       = {
    Wang, Benyou and Zhao, Donghao and Lioma, Christina and Li, Qiuchi and
    Zhang, Peng and Simonsen, Jakob Grue
  },
  year         = 2020,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=Hke-WTVtwr}
}
@inproceedings{Islam2020PosENet,
  title        = {How much Position Information Do Convolutional Neural Networks Encode?},
  author       = {Islam, Md Amirul and Jia, Sen and Bruce, Neil D. B.},
  year         = 2020,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=rJeB36NKvB}
}
@inproceedings{Liu2020FLOATER,
  title        = {
    Learning to Encode Position for Transformer with Continuous Dynamical Model
  },
  author       = {Liu, Xuanqing and Yu, Hsiang-Fu and Dhillon, Inderjit and Hsieh, Cho-Jui},
  year         = 2020,
  month        = {13--18 Jul},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 119,
  pages        = {6327--6335},
  url          = {https://proceedings.mlr.press/v119/liu20n.html},
  editor       = {III, Hal Daumé and Singh, Aarti},
  pdf          = {http://proceedings.mlr.press/v119/liu20n/liu20n.pdf},
  abstract     = {
    We introduce a new way of learning to encode position information for
    non-recurrent models, such as Transformer models. Unlike RNN and LSTM,
    which contain inductive bias by loading the input tokens sequentially,
    non-recurrent models are less sensitive to position. The main reason is
    that position information among input units is not encoded inherently,
    i.e., they are permutation equivalent, this problem justifies why all of
    the existing models are accompanied by position encoding/embedding layer at
    the input. However, this solution has clear limitations: the sinusoidal
    position encoding is not flexible enough as it is manually designed and
    does not contain any learnable parameters, whereas the position embedding
    restricts the maximum length of input sequences. It is thus desirable to
    design a new position layer that contains learnable parameters to adjust to
    different datasets and different architectures. At the same time, we would
    also like it to extrapolate in accordance with the variable length of
    inputs. In our proposed solution, we borrow from the recent Neural ODE
    approach, which may be viewed as a versatile continuous version of a
    ResNet. This model is capable of modeling many kinds of dynamical systems.
    We model the evolution of encoded results along position index by such a
    dynamical system, thereby overcoming the above limitations of existing
    methods. We evaluate our new position layers on a variety of neural machine
    translation and language understanding tasks, the experimental results show
    consistent improvements over the baselines.
  }
}
@inproceedings{SemihKayhan2020ASLConv,
  title        = {
    On Translation Invariance in CNNs: Convolutional Layers Can Exploit
    Absolute Spatial Location
  },
  author       = {Semih Kayhan, Osman and van Gemert, Jan C.},
  year         = 2020,
  month        = {June},
  booktitle    = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {14262--14273},
  doi          = {10.1109/CVPR42600.2020.01428},
  issn         = {2575-7075},
  abstract     = {
    In this paper we challenge the common assumption that convolutional layers
    in modern CNNs are translation invariant. We show that CNNs can and will
    exploit the absolute spatial location by learning filters that respond
    exclusively to particular absolute locations by exploiting image boundary
    effects. Because modern CNNs filters have a huge receptive field, these
    boundary effects operate even far from the image boundary, allowing the
    network to exploit absolute spatial location all over the image. We give a
    simple solution to remove spatial location encoding which improves
    translation invariance and thus gives a stronger visual inductive bias
    which particularly benefits small data sets. We broadly demonstrate these
    benefits on several architectures and various applications such as image
    classification, patch matching, and two video classification datasets.
  }
}
@inproceedings{Dai2019Transformer-XL,
  title        = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
  author       = {
    Dai, Zihang  and Yang, Zhilin  and Yang, Yiming  and Carbonell, Jaime  and
    Le, Quoc  and Salakhutdinov, Ruslan
  },
  year         = 2019,
  month        = jul,
  booktitle    = {
    Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics
  },
  publisher    = {Association for Computational Linguistics},
  address      = {Florence, Italy},
  pages        = {2978--2988},
  doi          = {10.18653/v1/P19-1285},
  url          = {https://aclanthology.org/P19-1285},
  abstract     = {
    Transformers have a potential of learning longer-term dependency, but are
    limited by a fixed-length context in the setting of language modeling. We
    propose a novel neural architecture Transformer-XL that enables learning
    dependency beyond a fixed length without disrupting temporal coherence. It
    consists of a segment-level recurrence mechanism and a novel positional
    encoding scheme. Our method not only enables capturing longer-term
    dependency, but also resolves the context fragmentation problem. As a
    result, Transformer-XL learns dependency that is 80{\%} longer than RNNs
    and 450{\%} longer than vanilla Transformers, achieves better performance
    on both short and long sequences, and is up to 1,800+ times faster than
    vanilla Transformers during evaluation. Notably, we improve the
    state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on
    text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn
    Treebank (without finetuning). When trained only on WikiText-103,
    Transformer-XL manages to generate reasonably coherent, novel text articles
    with thousands of tokens. Our code, pretrained models, and hyperparameters
    are available in both Tensorflow and PyTorch.
  }
}
@inproceedings{Loshchilov2018AdamW,
  title        = {Decoupled Weight Decay Regularization},
  author       = {Loshchilov, Ilya and Hutter, Frank},
  year         = 2019,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=Bkg6RiCqY7}
}
@inproceedings{Chen2018GradNorm,
  title        = {
    {G}rad{N}orm: Gradient Normalization for Adaptive Loss Balancing in Deep
    Multitask Networks
  },
  author       = {
    Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich,
    Andrew
  },
  year         = 2018,
  month        = {10--15 Jul},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {794--803},
  url          = {https://proceedings.mlr.press/v80/chen18a.html},
  editor       = {Dy, Jennifer and Krause, Andreas},
  pdf          = {http://proceedings.mlr.press/v80/chen18a/chen18a.pdf},
  abstract     = {
    Deep multitask networks, in which one neural network produces multiple
    predictive outputs, can offer better speed and performance than their
    single-task counterparts but are challenging to train properly. We present
    a gradient normalization (GradNorm) algorithm that automatically balances
    training in deep multitask models by dynamically tuning gradient
    magnitudes. We show that for various network architectures, for both
    regression and classification tasks, and on both synthetic and real
    datasets, GradNorm improves accuracy and reduces overfitting across
    multiple tasks when compared to single-task networks, static baselines, and
    other adaptive multitask loss balancing techniques. GradNorm also matches
    or surpasses the performance of exhaustive grid search methods, despite
    only involving a single asymmetry hyperparameter $\alpha$. Thus, what was
    once a tedious search process that incurred exponentially more compute for
    each task added can now be accomplished within a few training runs,
    irrespective of the number of tasks. Ultimately, we will demonstrate that
    gradient manipulation affords us great control over the training dynamics
    of multitask networks and may be one of the keys to unlocking the potential
    of multitask learning.
  }
}
@inproceedings{Trabelsi2018DCN,
  title        = {Deep Complex Networks},
  author       = {
    Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy
    and Subramanian, Sandeep and Santos, Joao Felipe and Mehri, Soroush and
    Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J
  },
  year         = 2018,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=H1T2hmZAb}
}
@inproceedings{Jacot2018NTK,
  title        = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author       = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
  year         = 2018,
  booktitle    = {
    Proceedings of the 32nd International Conference on Neural Information
    Processing Systems
  },
  location     = {Montr\'{e}al, Canada},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NIPS'18},
  pages        = {8580–8589},
  abstract     = {
    At initialization, artificial neural networks (ANNs) are equivalent to
    Gaussian processes in the infinite-width limit [12, 9], thus connecting
    them to kernel methods. We prove that the evolution of an ANN during
    training can also be described by a kernel: during gradient descent on the
    parameters of an ANN, the network function fθ (which maps input vectors to
    output vectors) follows the kernel gradient of the functional cost (which
    is convex, in contrast to the parameter cost) w.r.t. a new kernel: the
    Neural Tangent Kernel (NTK). This kernel is central to describe the
    generalization features of ANNs. While the NTK is random at initialization
    and varies during training, in the infinite-width limit it converges to an
    explicit limiting kernel and it stays constant during training. This makes
    it possible to study the training of ANNs in function space instead of
    parameter space. Convergence of the training can then be related to the
    positive-definiteness of the limiting NTK. We then focus on the setting of
    least-squares regression and show that in the infinite-width limit, the
    network function fθ follows a linear differential equation during training.
    The convergence is fastest along the largest kernel principal components of
    the input data with respect to the NTK, hence suggesting a theoretical
    motivation for early stopping.Finally we study the NTK numerically, observe
    its behavior for wide networks, and compare it to the infinite-width limit.
  },
  numpages     = 10
}
@inproceedings{Shaw2018RPE,
  title        = {Self-Attention with Relative Position Representations},
  author       = {Shaw, Peter  and Uszkoreit, Jakob  and Vaswani, Ashish},
  year         = 2018,
  month        = jun,
  booktitle    = {
    Proceedings of the 2018 Conference of the North {A}merican Chapter of the
    Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)
  },
  publisher    = {Association for Computational Linguistics},
  address      = {New Orleans, Louisiana},
  pages        = {464--468},
  doi          = {10.18653/v1/N18-2074},
  url          = {https://aclanthology.org/N18-2074},
  abstract     = {
    Relying entirely on an attention mechanism, the Transformer introduced by
    Vaswani et al. (2017) achieves state-of-the-art results for machine
    translation. In contrast to recurrent and convolutional neural networks, it
    does not explicitly model relative or absolute position information in its
    structure. Instead, it requires adding representations of absolute
    positions to its inputs. In this work we present an alternative approach,
    extending the self-attention mechanism to efficiently consider
    representations of the relative positions, or distances between sequence
    elements. On the WMT 2014 English-to-German and English-to-French
    translation tasks, this approach yields improvements of 1.3 BLEU and 0.3
    BLEU over absolute position representations, respectively. Notably, we
    observe that combining relative and absolute position representations
    yields no further improvement in translation quality. We describe an
    efficient implementation of our method and cast it as an instance of
    relation-aware self-attention mechanisms that can generalize to arbitrary
    graph-labeled inputs.
  }
}
@inproceedings{Gehring2017APE,
  title        = {Convolutional Sequence to Sequence Learning},
  author       = {
    Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and
    Dauphin, Yann N.
  },
  year         = 2017,
  month        = {06--11 Aug},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 70,
  pages        = {1243--1252},
  url          = {https://proceedings.mlr.press/v70/gehring17a.html},
  editor       = {Precup, Doina and Teh, Yee Whye},
  pdf          = {http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf},
  abstract     = {
    The prevalent approach to sequence to sequence learning maps an input
    sequence to a variable length output sequence via recurrent neural
    networks. We introduce an architecture based entirely on convolutional
    neural networks. Compared to recurrent models, computations over all
    elements can be fully parallelized during training to better exploit the
    GPU hardware and optimization is easier since the number of non-linearities
    is fixed and independent of the input length. Our use of gated linear units
    eases gradient propagation and we equip each decoder layer with a separate
    attention module. We outperform the accuracy of the deep LSTM setup of Wu
    et al. (2016) on both WMT’14 English-German and WMT’14 English-French
    translation at an order of magnitude faster speed, both on GPU and CPU.
  }
}
@inproceedings{Vaswani2017Transformer,
  title        = {Attention is All you Need},
  author       = {
    Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and
    Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia
  },
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 30,
  url          = {
    https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
  },
  editor       = {
    I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and
    S. Vishwanathan and R. Garnett
  }
}
@inproceedings{Jaderberg2015STN,
  title        = {Spatial Transformer Networks},
  author       = {
    Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and kavukcuoglu,
    koray
  },
  year         = 2015,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 28,
  url          = {
    https://proceedings.neurips.cc/paper_files/paper/2015/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf
  },
  editor       = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett}
}
@article{Reichert2013Neuronal,
  title        = {Neuronal Synchrony in Complex-Valued Deep Networks},
  author       = {Reichert, David P. and Serre, Thomas},
  year         = 2013,
  journal      = {International Conference On Learning Representations},
  bibsource    = {
    Semantic Scholar
    https://www.semanticscholar.org/paper/7c5920c97f8bb1f91739b0d27746d655de95eedd
  }
}
@inproceedings{Sutskever2013SGD,
  title        = {On the importance of initialization and momentum in deep learning},
  author       = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year         = 2013,
  month        = {17--19 Jun},
  booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {Atlanta, Georgia, USA},
  series       = {Proceedings of Machine Learning Research},
  volume       = 28,
  number       = 3,
  pages        = {1139--1147},
  url          = {https://proceedings.mlr.press/v28/sutskever13.html},
  editor       = {Dasgupta, Sanjoy and McAllester, David},
  pdf          = {http://proceedings.mlr.press/v28/sutskever13.pdf},
  abstract     = {
    Deep and recurrent neural networks (DNNs and RNNs respectively) are
    powerful models that were considered to be almost impossible to train using
    stochastic gradient descent with momentum. In this paper, we show that when
    stochastic gradient descent with momentum uses a well-designed random
    initialization and a particular type of slowly increasing schedule for the
    momentum parameter, it can train both DNNs and RNNs (on datasets with
    long-term dependencies) to levels of performance that were previously
    achievable only with Hessian-Free optimization. We find that both the
    initialization and the momentum are crucial since poorly initialized
    networks cannot be trained with momentum and well-initialized networks
    perform markedly worse when the momentum is absent or poorly tuned.     Our
    success training these models suggests that previous attempts to train deep
    and recurrent neural networks from random initializations have likely
    failed due to poor initialization schemes. Furthermore, carefully tuned
    momentum methods suffice for dealing with the curvature issues in deep and
    recurrent network training objectives without the need for sophisticated
    second-order methods.
  }
}
@article{Goto1954Parametron,
  title        = {The parametron--A new circuit element which utilizes non-linear reactors},
  author       = {Goto, Eiichi},
  year         = 1954,
  journal      = {
    Paper of Technical Group of Electronic Computers and Nonlinear Theory, IECE
    (July 1954)(in Japanese)
  }
}
