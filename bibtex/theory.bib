@article{Scardapane2020CVNNNAF,
  title        = {Complex-Valued Neural Networks With Nonparametric Activation Functions},
  author       = {
    Scardapane, Simone and Van Vaerenbergh, Steven and Hussain, Amir and
    Uncini, Aurelio
  },
  year         = 2020,
  month        = {April},
  journal      = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume       = 4,
  number       = 2,
  pages        = {140--150},
  doi          = {10.1109/TETCI.2018.2872600},
  issn         = {2471-285X},
  abstract     = {
    Complex-valued neural networks (CVNNs) are a powerful modeling tool for
    domains where data can be naturally interpreted in terms of complex
    numbers. However, several analytical properties of the complex domain (such
    as holomorphicity) make the design of CVNNs a more challenging task than
    their real counterpart. In this paper, we consider the problem of flexible
    activation functions (AFs) in the complex domain, i.e., AFs endowed with
    sufficient degrees of freedom to adapt their shape given the training data.
    While this problem has received considerable attention in the real case,
    very limited literature exists for CVNNs, where most activation functions
    are generally developed in a split fashion (i.e., by considering the real
    and imaginary parts of the activation separately) or with simple
    phase-amplitude techniques. Leveraging over the recently proposed kernel
    activation functions, and related advances in the design of complex-valued
    kernels, we propose the first fully complex, nonparametric activation
    function for CVNNs, which is based on a kernel expansion with a fixed
    dictionary that can be implemented efficiently on vectorized hardware.
    Several experiments on common use cases, including prediction and channel
    equalization, validate our proposal when compared to real-valued neural
    networks and CVNNs with fixed activation functions.
  },
}
@inproceedings{Chen2018GradNorm,
  title        = {
    {G}rad{N}orm: Gradient Normalization for Adaptive Loss Balancing in Deep
    Multitask Networks
  },
  author       = {
    Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich,
    Andrew
  },
  year         = 2018,
  month        = {10--15 Jul},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {794--803},
  url          = {https://proceedings.mlr.press/v80/chen18a.html},
  editor       = {Dy, Jennifer and Krause, Andreas},
  pdf          = {http://proceedings.mlr.press/v80/chen18a/chen18a.pdf},
  abstract     = {
    Deep multitask networks, in which one neural network produces multiple
    predictive outputs, can offer better speed and performance than their
    single-task counterparts but are challenging to train properly. We present
    a gradient normalization (GradNorm) algorithm that automatically balances
    training in deep multitask models by dynamically tuning gradient
    magnitudes. We show that for various network architectures, for both
    regression and classification tasks, and on both synthetic and real
    datasets, GradNorm improves accuracy and reduces overfitting across
    multiple tasks when compared to single-task networks, static baselines, and
    other adaptive multitask loss balancing techniques. GradNorm also matches
    or surpasses the performance of exhaustive grid search methods, despite
    only involving a single asymmetry hyperparameter $\alpha$. Thus, what was
    once a tedious search process that incurred exponentially more compute for
    each task added can now be accomplished within a few training runs,
    irrespective of the number of tasks. Ultimately, we will demonstrate that
    gradient manipulation affords us great control over the training dynamics
    of multitask networks and may be one of the keys to unlocking the potential
    of multitask learning.
  }
}
@inproceedings{Jacot2018NTK,
  title        = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author       = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
  year         = 2018,
  booktitle    = {
    Proceedings of the 32nd International Conference on Neural Information
    Processing Systems
  },
  location     = {Montr\'{e}al, Canada},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NIPS'18},
  pages        = {8580–8589},
  abstract     = {
    At initialization, artificial neural networks (ANNs) are equivalent to
    Gaussian processes in the infinite-width limit [12, 9], thus connecting
    them to kernel methods. We prove that the evolution of an ANN during
    training can also be described by a kernel: during gradient descent on the
    parameters of an ANN, the network function fθ (which maps input vectors to
    output vectors) follows the kernel gradient of the functional cost (which
    is convex, in contrast to the parameter cost) w.r.t. a new kernel: the
    Neural Tangent Kernel (NTK). This kernel is central to describe the
    generalization features of ANNs. While the NTK is random at initialization
    and varies during training, in the infinite-width limit it converges to an
    explicit limiting kernel and it stays constant during training. This makes
    it possible to study the training of ANNs in function space instead of
    parameter space. Convergence of the training can then be related to the
    positive-definiteness of the limiting NTK. We then focus on the setting of
    least-squares regression and show that in the infinite-width limit, the
    network function fθ follows a linear differential equation during training.
    The convergence is fastest along the largest kernel principal components of
    the input data with respect to the NTK, hence suggesting a theoretical
    motivation for early stopping.Finally we study the NTK numerically, observe
    its behavior for wide networks, and compare it to the infinite-width limit.
  },
  numpages     = 10
}
