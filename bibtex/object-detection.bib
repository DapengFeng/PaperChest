@misc{Munir2023BPC,
  title        = {
    Bridging Precision and Confidence: A Train-Time Loss for Calibrating Object
    Detection
  },
  author       = {
    Munir, Muhammad Akhtar and Khan, Muhammad Haris and Khan, Salman and Khan,
    Fahad Shahbaz
  },
  year         = 2023,
  eprint       = {2303.14404},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@misc{Zhu2023KCR,
  title        = {
    Knowledge Combination to Learn Rotated Detection Without Rotated Annotation
  },
  author       = {
    Zhu, Tianyu and Ferenczi, Bryce and Purkait, Pulak and Drummond, Tom and
    Rezatofighi, Hamid and Hengel, Anton van den
  },
  year         = 2023,
  eprint       = {2304.02199},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@inproceedings{Guo2022Hire-MLP,
  title        = {Hire-MLP: Vision MLP via Hierarchical Rearrangement},
  author       = {
    Guo, Jianyuan and Tang, Yehui and Han, Kai and Chen, Xinghao and Wu, Han
    and Xu, Chao and Xu, Chang and Wang, Yunhe
  },
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {816--826},
  doi          = {10.1109/CVPR52688.2022.00090},
  issn         = {2575-7075},
  abstract     = {
    Previous vision MLPs such as MLP-Mixer and ResMLP accept linearly flattened
    image patches as input, making them inflexible for different input sizes
    and hard to capture spatial information. Such approach withholds MLPs from
    getting comparable performance with their transformer-based counterparts
    and prevents them from becoming a general backbone for computer vision.
    This paper presents Hire-MLP, a simple yet competitive vision MLP
    architecture via Hierarchical rearrangement, which contains two levels of
    rearrangements. Specifically, the inner-region rearrangement is proposed to
    capture local information inside a spatial region, and the cross-region
    rearrangement is proposed to enable information communication between
    different regions and capture global context by circularly shifting all
    tokens along spatial directions. Extensive experiments demonstrate the
    effectiveness of Hire-MLP as a versatile backbone for various vision tasks.
    In particular, Hire-MLP achieves competitive results on image
    classification, object detection and semantic segmentation tasks, e.g.,
    83.8% top-1 accuracy on ImageNet, 51.7% box AP and 44.8% mask AP on COCO
    val2017, and 49.9% mIoU on ADE20K, surpassing previous transformer-based
    and MLP-based models with better trade-off for accuracy and throughput.
  }
}
@inproceedings{Liu2022SwinTransformerV2,
  title        = {Swin Transformer V2: Scaling Up Capacity and Resolution},
  author       = {
    Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and
    Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and
    Wei, Furu and Guo, Baining
  },
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {11999--12009},
  doi          = {10.1109/CVPR52688.2022.01170},
  issn         = {2575-7075},
  abstract     = {
    We present techniques for scaling Swin Transformer [35] up to 3 billion
    parameters and making it capable of training with images of up to
    1,536x1,536 resolution. By scaling up capacity and resolution, Swin
    Transformer sets new records on four representative vision benchmarks:
    84.0% top-1 accuracy on ImageNet- V2 image classification, 63.1 / 54.4 box
    / mask mAP on COCO object detection, 59.9 mIoU on ADE20K semantic
    segmentation, and 86.8% top-1 accuracy on Kinetics-400 video action
    classification. We tackle issues of training instability, and study how to
    effectively transfer models pre-trained at low resolutions to higher
    resolution ones. To this aim, several novel technologies are proposed: 1) a
    residual post normalization technique and a scaled cosine attention
    approach to improve the stability of large vision models; 2) a log-spaced
    continuous position bias technique to effectively transfer models
    pre-trained at low-resolution images and windows to their higher-resolution
    counterparts. In addition, we share our crucial implementation details that
    lead to significant savings of GPU memory consumption and thus make it
    feasi-ble to train large vision models with regular GPUs. Using these
    techniques and self-supervised pre-training, we suc-cessfully train a
    strong 3 billion Swin Transformer model and effectively transfer it to
    various vision tasks involving high-resolution images or windows, achieving
    the state-of-the-art accuracy on a variety of benchmarks. Code is
    avail-able at https://github.com/microsoft/Swin-Transformer.
  }
}
@inproceedings{Tang2022Wave-MLP,
  title        = {An Image Patch is a Wave: Phase-Aware Vision MLP},
  author       = {
    Tang, Yehui and Han, Kai and Guo, Jianyuan and Xu, Chang and Li, Yanxi and
    Xu, Chao and Wang, Yunhe
  },
  year         = 2022,
  month        = {June},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {10925--10934},
  doi          = {10.1109/CVPR52688.2022.01066},
  issn         = {2575-7075},
  abstract     = {
    In the field of computer vision, recent works show that a pure MLP
    architecture mainly stacked by fully-connected layers can achieve competing
    performance with CNN and transformer. An input image of vision MLP is
    usually split into multiple tokens (patches), while the existing MLP models
    directly aggregate them with fixed weights, neglecting the varying semantic
    information of tokens from different images. To dynamically aggregate
    tokens, we propose to represent each token as a wave function with two
    parts, amplitude and phase. Amplitude is the original feature and the phase
    term is a complex value changing according to the semantic contents of
    input images. Introducing the phase term can dynamically modulate the
    relationship between tokens and fixed weights in MLP. Based on the
    wave-like token representation, we establish a novel Wave-MLP architecture
    for vision tasks. Extensive experiments demonstrate that the proposed
    Wave-MLP is superior to the state-of-the-art MLP architectures on various
    vision tasks such as image classification, object detection and semantic
    segmentation. The source code is available at
    https://github.com/huawei-noah/CV-Backbones/tree/master/wavemlp_pytorch and
    https://gitee.com/mindspore/models/tree/master/research/cv/wave_mlp.
  }
}
@article{Yang2022KFIoU,
  title        = {The KFIoU Loss for Rotated Object Detection},
  author       = {
    Xue Yang and Yue Zhou and Gefan Zhang and Jirui Yang and Wentao Wang and
    Junchi Yan and Xiaopeng Zhang and Qi Tian
  },
  year         = 2022,
  journal      = {arXiv preprint arXiv: Arxiv-2201.12558}
}
@inproceedings{Guo2021CFA,
  title        = {
    Beyond Bounding-Box: Convex-hull Feature Adaptation for Oriented and
    Densely Packed Object Detection
  },
  author       = {
    Guo, Zonghao and Liu, Chang and Zhang, Xiaosong and Jiao, Jianbin and Ji,
    Xiangyang and Ye, Qixiang
  },
  year         = 2021,
  month        = {June},
  booktitle    = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {8788--8797},
  doi          = {10.1109/CVPR46437.2021.00868},
  issn         = {2575-7075},
  abstract     = {
    Detecting oriented and densely packed objects remains challenging for
    spatial feature aliasing caused by the intersection of reception fields
    between objects. In this paper, we propose a convex-hull feature adaptation
    (CFA) approach for configuring convolutional features in accordance with
    oriented and densely packed object layouts. CFA is rooted in convex-hull
    feature representation, which defines a set of dynamically predicted
    feature points guided by the convex intersection over union (CIoU) to bound
    the extent of objects. CFA pursues optimal feature assignment by
    constructing convex-hull sets and dynamically splitting positive or
    negative convex-hulls. By simultaneously considering overlapping
    convex-hulls and objects and penalizing convex-hulls shared by multiple
    objects, CFA alleviates spatial feature aliasing towards optimal feature
    adaptation. Experiments on DOTA and SKU110K-R datasets show that CFA
    significantly outperforms the baseline approach, achieving new
    state-of-the-art detection performance. Code is available at
    github.com/SDL-GuoZonghao/BeyondBoundingBox.
  }
}
@inproceedings{Liu2021SwinTransformer,
  title        = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author       = {
    Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang,
    Zheng and Lin, Stephen and Guo, Baining
  },
  year         = 2021,
  month        = {Oct},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages        = {9992--10002},
  doi          = {10.1109/ICCV48922.2021.00986},
  issn         = {2380-7504},
  abstract     = {
    This paper presents a new vision Transformer, called Swin Transformer, that
    capably serves as a general-purpose backbone for computer vision.
    Challenges in adapting Transformer from language to vision arise from
    differences between the two domains, such as large variations in the scale
    of visual entities and the high resolution of pixels in images compared to
    words in text. To address these differences, we propose a hierarchical
    Transformer whose representation is computed with Shifted windows. The
    shifted windowing scheme brings greater efficiency by limiting
    self-attention computation to non-overlapping local windows while also
    allowing for cross-window connection. This hierarchical architecture has
    the flexibility to model at various scales and has linear computational
    complexity with respect to image size. These qualities of Swin Transformer
    make it compatible with a broad range of vision tasks, including image
    classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction
    tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO
    test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its
    performance surpasses the previous state-of-the-art by a large margin of
    +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K,
    demonstrating the potential of Transformer-based models as vision
    backbones. The hierarchical design and the shifted window approach also
    prove beneficial for all-MLP architectures. The code and models are
    publicly available at https://github.com/microsoft/Swin-Transformer.
  }
}
@inproceedings{Tolstikhin2021MLP-Mixer,
  title        = {MLP-Mixer: An all-MLP Architecture for Vision},
  author       = {
    Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer,
    Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and
    Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario
    and Dosovitskiy, Alexey
  },
  year         = 2021,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 34,
  pages        = {24261--24272},
  url          = {
    https://proceedings.neurips.cc/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf
  },
  editor       = {
    M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman
    Vaughan
  }
}
@inproceedings{Yang2021GWD,
  title        = {
    Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss
  },
  author       = {
    Yang, Xue and Yan, Junchi and Ming, Qi and Wang, Wentao and Zhang, Xiaopeng
    and Tian, Qi
  },
  year         = 2021,
  month        = {18--24 Jul},
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 139,
  pages        = {11830--11841},
  url          = {https://proceedings.mlr.press/v139/yang21l.html},
  editor       = {Meila, Marina and Zhang, Tong},
  pdf          = {http://proceedings.mlr.press/v139/yang21l/yang21l.pdf},
  abstract     = {
    Boundary discontinuity and its inconsistency to the final detection metric
    have been the bottleneck for rotating detection regression loss design. In
    this paper, we propose a novel regression loss based on Gaussian
    Wasserstein distance as a fundamental approach to solve the problem.
    Specifically, the rotated bounding box is converted to a 2-D Gaussian
    distribution, which enables to approximate the indifferentiable rotational
    IoU induced loss by the Gaussian Wasserstein distance (GWD) which can be
    learned efficiently by gradient back-propagation. GWD can still be
    informative for learning even there is no overlapping between two rotating
    bounding boxes which is often the case for small object detection. Thanks
    to its three unique properties, GWD can also elegantly solve the boundary
    discontinuity and square-like problem regardless how the bounding box is
    defined. Experiments on five datasets using different detectors show the
    effectiveness of our approach, and codes are available at
    https://github.com/yangxue0827/RotationDetection.
  }
}
@inproceedings{Yang2021KLD,
  title        = {
    Learning High-Precision Bounding Box for Rotated Object Detection via
    Kullback-Leibler Divergence
  },
  author       = {
    Yang, Xue and Yang, Xiaojiang and Yang, Jirui and Ming, Qi and Wang, Wentao
    and Tian, Qi and Yan, Junchi
  },
  year         = 2021,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 34,
  pages        = {18381--18394},
  url          = {
    https://proceedings.neurips.cc/paper/2021/file/98f13708210194c475687be6106a3b84-Paper.pdf
  },
  editor       = {
    M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman
    Vaughan
  }
}
@inproceedings{Zhu2021FCENet,
  title        = {Fourier Contour Embedding for Arbitrary-Shaped Text Detection},
  author       = {
    Zhu, Yiqin and Chen, Jianyong and Liang, Lingyu and Kuang, Zhanghui and
    Jin, Lianwen and Zhang, Wayne
  },
  year         = 2021,
  month        = {June},
  booktitle    = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {3122--3130},
  doi          = {10.1109/CVPR46437.2021.00314},
  issn         = {2575-7075},
  abstract     = {
    One of the main challenges for arbitrary-shaped text detection is to design
    a good text instance representation that allows networks to learn diverse
    text geometry variances. Most of existing methods model text instances in
    image spatial domain via masks or contour point sequences in the Cartesian
    or the polar coordinate system. However, the mask representation might lead
    to expensive post-processing, while the point sequence one may have limited
    capability to model texts with highly-curved shapes. To tackle these
    problems, we model text instances in the Fourier domain and propose one
    novel Fourier Contour Embedding (FCE) method to represent arbitrary shaped
    text contours as compact signatures. We further construct FCENet with a
    backbone, feature pyramid networks (FP-N) and a simple post-processing with
    the Inverse Fourier Transformation (IFT) and Non-Maximum Suppression
    (N-MS). Different from previous methods, FCENet first pre-dicts compact
    Fourier signatures of text instances, and then reconstructs text contours
    via IFT and NMS during test. Extensive experiments demonstrate that FCE is
    accurate and robust to fit contours of scene texts even with highly-curved
    shapes, and also validate the effectiveness and the good generalization of
    FCENet for arbitrary-shaped text detection. Furthermore, experimental
    results show that our FCENet is superior to the state-of-the-art (SOTA)
    meth-ods on CTW1500 and Total-Text, especially on challenging highly-curved
    text subset.
  }
}
@inproceedings{Chen2020RepPointsV2,
  title        = {RepPoints v2: Verification Meets Regression for Object Detection},
  author       = {
    Chen, Yihong and Zhang, Zheng and Cao, Yue and Wang, Liwei and Lin, Stephen
    and Hu, Han
  },
  year         = 2020,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 33,
  pages        = {5621--5631},
  url          = {
    https://proceedings.neurips.cc/paper/2020/file/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Paper.pdf
  },
  editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@inproceedings{Yang2020DenseRepPoints,
  title        = {Dense RepPoints: Representing Visual Objects with Dense Point Sets},
  author       = {
    Yang, Ze and Xu, Yinghao and Xue, Han and Zhang, Zheng and Urtasun, Raquel
    and Wang, Liwei and Lin, Stephen and Hu, Han
  },
  year         = 2020,
  booktitle    = {Computer Vision -- ECCV 2020},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  pages        = {227--244},
  isbn         = {978-3-030-58589-1},
  editor       = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  abstract     = {
    We present a new object representation, called Dense RepPoints, that
    utilizes a large set of points to describe an object at multiple levels,
    including both box level and pixel level. Techniques are proposed to
    efficiently process these dense points, maintaining near-constant
    complexity with increasing point numbers. Dense RepPoints is shown to
    represent and learn object segments well, with the use of a novel distance
    transform sampling method combined with set-to-set supervision. The
    distance transform sampling combines the strengths of contour and grid
    representations, leading to performance that surpasses counterparts based
    on contours or grids. Code is available at
    https://github.com/justimyhxu/Dense-RepPoints.
  }
}
@inproceedings{Yang2019RepPoints,
  title        = {RepPoints: Point Set Representation for Object Detection},
  author       = {Yang, Ze and Liu, Shaohui and Hu, Han and Wang, Liwei and Lin, Stephen},
  year         = 2019,
  month        = {Oct},
  booktitle    = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages        = {9656--9665},
  doi          = {10.1109/ICCV.2019.00975},
  issn         = {2380-7504},
  abstract     = {
    Modern object detectors rely heavily on rectangular bounding boxes, such as
    anchors, proposals and the final predictions, to represent objects at
    various recognition stages. The bounding box is convenient to use but
    provides only a coarse localization of objects and leads to a
    correspondingly coarse extraction of object features. In this paper, we
    present RepPoints (representative points), a new finer representation of
    objects as a set of sample points useful for both localization and
    recognition. Given ground truth localization and recognition targets for
    training, RepPoints learn to automatically arrange themselves in a manner
    that bounds the spatial extent of an object and indicates semantically
    significant local areas. They furthermore do not require the use of anchors
    to sample a space of bounding boxes. We show that an anchor-free object
    detector based on RepPoints can be as effective as the state-of-the-art
    anchor-based detection methods, with 46.5 AP and 67.4 AP50 on the COCO
    test-dev detection benchmark, using ResNet-101 model. Code is available at
    https://github.com/microsoft/RepPoints.
  }
}
@article{Yan2018Second,
  title        = {SECOND: Sparsely Embedded Convolutional Detection},
  author       = {Yan, Yan and Mao, Yuxing and Li, Bo},
  year         = 2018,
  journal      = {Sensors},
  volume       = 18,
  number       = 10,
  doi          = {10.3390/s18103337},
  issn         = {1424-8220},
  url          = {https://www.mdpi.com/1424-8220/18/10/3337},
  article-number = 3337,
  pubmedid     = 30301196,
  abstract     = {
    LiDAR-based or RGB-D-based object detection is used in numerous
    applications, ranging from autonomous driving to robot vision. Voxel-based
    3D convolutional networks have been used for some time to enhance the
    retention of information when processing point cloud LiDAR data. However,
    problems remain, including a slow inference speed and low orientation
    estimation performance. We therefore investigate an improved sparse
    convolution method for such networks, which significantly increases the
    speed of both training and inference. We also introduce a new form of angle
    loss regression to improve the orientation estimation performance and a new
    data augmentation approach that can enhance the convergence speed and
    performance. The proposed network produces state-of-the-art results on the
    KITTI 3D object detection benchmarks while maintaining a fast inference
    speed.
  }
}
@inproceedings{Lin2017FPN,
  title        = {Feature Pyramid Networks for Object Detection},
  author       = {
    Lin, Tsung-Yi and Doll√°r, Piotr and Girshick, Ross and He, Kaiming and
    Hariharan, Bharath and Belongie, Serge
  },
  year         = 2017,
  month        = {July},
  booktitle    = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {936--944},
  doi          = {10.1109/CVPR.2017.106},
  issn         = {1063-6919},
  abstract     = {
    Feature pyramids are a basic component in recognition systems for detecting
    objects at different scales. But pyramid representations have been avoided
    in recent object detectors that are based on deep convolutional networks,
    partially because they are slow to compute and memory intensive. In this
    paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep
    convolutional networks to construct feature pyramids with marginal extra
    cost. A top-down architecture with lateral connections is developed for
    building high-level semantic feature maps at all scales. This architecture,
    called a Feature Pyramid Network (FPN), shows significant improvement as a
    generic feature extractor in several applications. Using a basic Faster
    R-CNN system, our method achieves state-of-the-art single-model results on
    the COCO detection benchmark without bells and whistles, surpassing all
    existing single-model entries including those from the COCO 2016 challenge
    winners. In addition, our method can run at 5 FPS on a GPU and thus is a
    practical and accurate solution to multi-scale object detection. Code will
    be made publicly available.
  }
}
